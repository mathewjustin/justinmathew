[{"content":"Solid Principles \u0026ldquo;Complexity is the enemy of execution.\u0026rdquo; — Tony Robbins(From a youtube short)\nMost of the times complexity in code, or complex explanation of a simple problem leads to failure in the longterm. If you can't keep things simple, or explain things simply, either you have not understood it fully or you are trying to make an impression of being smart. At times I used to act like I know something which I dont have a clue about. This is may be a defense mechanism to hide my insecurities. But there comes a day when someone asks you a question which you cant answer and you feel embarrassed. Why am I saying all these, we are just talking about solid principles. I have this stupid habbit of making things complex than it should be, may be that\u0026rsquo;s why i should read SOLID principles again, and try to make the code little simple for humans and AI to understand.\nTable of Contents Single Responsibility Principle (SRP) Open/Closed Principle (OCP) Liskov Substitution Principle (LSP) Interface Segregation Principle (ISP) Dependency Inversion Principle (DIP) Single Responsibility Principle We all might have seen a person who can do many things at a time but not really good in any one thing. For example, I\u0026rsquo;ve been a mediocre fullstack developer for long time, there was a time i thought i fully understand how DOM works and how javascript manipulate DOM, at the same time i had this false impression that I can possibly find a memory leak in a legacy java application written 20 years ago. The truth is I was not really good at any one thing.\nThis is somewhat happening in code as well. A big ass file which does way too many things. I am someone who believes that the name of a class or a file should reflect its responsibility. If a class or a file is doing too many things, it\u0026rsquo;s a sign that it needs to be refactored into smaller, more focused components.\nAs usually lets take an example. Unlike well defined examples which AI generates, i would like to make something my own. Lets take this example of an e-commerce appliaction. We all might have came across this class called Order.\nclass Order { constructor(id, items) { this.id = id; this.items = items; } calculateTotal() { return this.items.reduce((total, item) =\u0026gt; total + item.price, 0); } save() { // Save order to database } sendConfirmationEmail() { // Send confirmation email to customer } } This class at a glance look really perfect. See another example,\nclass OrderPostProcessor { constructor(order) { this.order = order; } process() { this.order.save(); this.order.sendConfirmationEmail(); } } At first glance this looks good, you have a class to control the order processing. If we look carefully and understand the reasoning behing SRP, it is to avoid motivation for change. If we need to change the way we save order, or send email, we will have to change the Order class. This is a violation of SRP.\nIf there is a logic change in email sending, we should not have to touch the Order class. So we can refactor the code like this.\nclass Order { constructor(id, items) { this.id = id; this.items = items; } calculateTotal() { return this.items.reduce((total, item) =\u0026gt; total + item.price, 0); } } class OrderRepository { save(order) { // Save to DB } } class EmailService { sendOrderConfirmation(order) { // Send email } } class OrderPostProcessor { constructor(orderRepo, emailService) { this.orderRepo = orderRepo; this.emailService = emailService; } process(order) { this.orderRepo.save(order); this.emailService.sendOrderConfirmation(order); } } Well what about the Order processing class? No it doen\u0026rsquo;t violate SRP because its only responsible for post processing the order, not how to save or send email. Although it looks like more code, it avoids the need for changing the Order class when there is a problem in email sending or saving to DB.\nWe will visit the remaing in the coming days. For now this is enough to understand SRP.\nOpen/Closed Principle Just like we read about the SRP, I\u0026rsquo;ve collected some qoutes on open closed principles. You know why I collected them? because those one liners has then entire point of OCP. Our aim in the coming years to make sure our code is being understood by AI and AI will make only minimal changes to the code. We might have seen AI regenerating the entire code base with just one slight prompt. Well that\u0026rsquo;s something which we should be avoiding. Our code should be written in a way that it is open for extension and closed for modification. Just like SRP the goal is to avoid motivation for change.\nSo the quotes again:\n\u0026ldquo;If adding a feature means touching stable code, you’re probably violating OCP\u0026rdquo; — I stole it from somewhere.\nThis is so true in case of legacy applications. I\u0026rsquo;ve worked on some very old projects where adding a small change is considered to be risky and bring in regressions. This mostly happens when the code is closed for extensions. There were cases where we put hundreds of if else conditions, run time flags to enable or disable a features. Run-time flags were the crazy ones we start with one flag then we end up having this flag for every small feature. Imagine there is an extension feature for feature A. What do we do? We add another flag. This leads to a combinatorial explosion of flags and conditions, making the codebase even more complex and harder to maintain. On legacy software products the first thing to identify before adding a new feature is to find out how to make a runtime flag for it. This is a clear sign of OCP violation.\nI know I should\u0026rsquo;t be preaching something which i practice less, But I am trying to get better at it.\nSide note: I am oversimplifying OCP here, we don\u0026rsquo;t write this in production grade code. When we write code in production if its just an email sending service, it will incluide retries, proper logging, metrics, outbox pattern..etc and etc. But this is just an evening read, and enough to crack the idea of OCP.\nSo lets take an example of the emailService we created in the previous section.\nsendOrderConfirmation(order) { // Send email } } So now this is just about sending an email. What if we have multiple email providers? For example, SendGrid, Amazon SES, SMTP etc. If we need to add support for multiple email providers, we should not be modifying the EmailService class. Instead we can create an interface for EmailProvider and implement different providers.\ninterface EmailProvider { send(email) { throw new Error(\u0026#34;Method \u0026#39;send()\u0026#39; must be implemented.\u0026#34;); } } class SendGridProvider implements EmailProvider { send(email) { // Send email using SendGrid } } class AmazonSESProvider implements EmailProvider { send(email) { // Send email using Amazon SES } } class EmailService { constructor(provider) { this.provider = provider; } sendOrderConfirmation(order) { const email = this.createEmail(order); this.provider.send(email); } createEmail(order) { // Create email content } } For example in Spring boot we can have multiple implementations for email service, and based on lets say which IaaS we are deploying the application we can choose the implementation at runtime using dependency injection. This is very usual for example the same app we deploy on AWS might use Amazon SES provider, while the one on Azure might use SendGrid provider.\nUsually it happens for applications which are deployed on multiple cloud providers, like SaaS products. I am fortunate to have worked on such products. Where the same code will be implemented differently based on the cloud provider.\nNow the above code is open for extension, we can add more providers without modifying the EmailService class. This is the essence of OCP.\nLiskov Substitution Principle You know what, this name Liskov substitution principle is sometimes intemidating. Such a facncy name. But the idea is simple. Look at this image which i stole from the globalnerdy.\nWell if i put this into words. In mathematics Square \u0026ldquo;is a\u0026rdquo; Rectangle. You are getting the point..? \u0026ldquo;is a\u0026rdquo;. So based on this you can make square from rectangle, what could go wrong in this? Let\u0026rsquo;s just write it down..\nclass Rectangle { constructor(width, height) { this.width = width; this.height = height; } setWidth(width) { this.width = width; } setHeight(height) { this.height = height; } getArea() { return this.width * this.height; } } class Square extends Rectangle { constructor(side) { super(side, side); } setWidth(width) { this.width = width; this.height = width; // Square must have equal sides } setHeight(height) { this.width = height; this.height = height; // Square must have equal sides } } So now Mathematically yes square is a rectangle, But this code? hmmm\u0026hellip; It has a problem. This violates Liskoves substitution principle. Why?\nThe moment you replace Square at the place you want to use Rectangle, you will mess up the code. Liskov substitution says you should be substitutable from the classes it derived. Can i substiture Rectangle with Square in this case? No we cant.\nThe moral of the story is : model your classes based on behaviours not on properties; model your data based on properties and not on behaviours. If it behaves like a duck, it\u0026rsquo;s certainly a bird.\nInterface Segregation Principle Dependency Inversion Principle ","permalink":"http://localhost:1313/posts/super-solid/","summary":"\u003ch1 id=\"solid-principles\"\u003eSolid Principles\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;Complexity is the enemy of execution.\u0026rdquo; — Tony Robbins(From a youtube short)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMost of the times complexity in code, or complex explanation of a simple problem leads to failure in the longterm. If you can't keep things simple, or explain things simply, either you have not understood it fully or you are trying to make an impression of being smart. At times I used to act like I know something which I dont have a clue about. This is may be a defense mechanism to hide my insecurities. But there comes a day when someone asks you a question which you cant answer and you feel embarrassed.\n\u003c/p\u003e","title":"Solid Principles"},{"content":"Introduction B-Trees are a facinating inventions in computing. They are some special kind of binary tree. But with very concrete rules. I have watched small video and now i am in the b-tree rabbit hole. I\u0026rsquo;ts nice last time i\u0026rsquo;ve been in a rabbit hole like this was when i was learning about prime numbers and how they used in cryptography, well thats a blog post for another day.\nSo The btree data structure in simple terms is a tree which has multiple values in single node, and children nodes can be more than 2. In binary tree we have only 2 children nodes, but in btree we can have more than 2 children nodes.\nIn binary tree the rule is to the left of the root node we have smaller values and to the right of the root node we have larger values compared to the root node, but in the btree the childrens are on a range of values and the range is defined by the parent node.\nThe most interesting part of btree is the rules which it has, why the rules? Because we wanted a datastructure which can do fast search, insert and delete operations. And the btree is designed to do that. To achieve this there are certain rules which is present in btree.\nLets start with the below binary tree. 19 / \\ 7 37 / \\ / \\ 3 13 29 43 / \\ / \\ / \\ / \\ 2 5 11 17 23 31 41 47 Just think about the searching in this tree, at any given point we always search on half of this tree. Either left side or right side. So the time complexity is logermic. So at this point we may feel this is really good, but think about this you have a massive amout of indexes and page poiters to eb stored in case of postgresql or mysql. And if it was binary tree then the depth of the tree will be more and more. So the search will take more time, becase even if we are searching on half of the tree, the depth of the tree is more.\nSo B trees will help us to reduce the depth of the tree, and it helps to reach the index faster. Lets see the above tree how is it represented as a btree.\nSo imagine this - Instead of spliting into half, like left child and right child, can we split into 3 and reduce the search space into 1/3, what if we split into 4 we reduce the search space to 1/4..so on. Something like below:\n[13, 29] / | \\ [5, 7] [17, 23] [37, 43] / \\ | / | \\ [2, 3] [11] [19] [31] [41] [47] Values less than the first key of the root node will be to the left Values greater than the second key of the root node will be to the right. And, the values which are between first and second key will be on the middle. This is how it is represented. Now we need to look at less number of nodes to get to the answer. Now we may think there are way too many comparisons than binary tree, but imagine this. The most time spent is not on comparing but while fetching the data to be compared. In case of databases it is completely true.\nRules: Certain rules which needs to keep in mind are.\nB-Tree is a balanced tree, which means all leaf nodes are at the same level. Each node can have a variable number of keys and children. The keys in each node are sorted in ascending order. The number of keys in a node is between a predefined minimum and maximum. The root node must have at least one key. So the Primary logic of B-Tree is to keep this rules intact while performing insert and delete operations.\nInsert? For example while inserting a new key if the node is full then we need to split the node into two nodes and promote the middle key to the parent node. This will keep the tree balanced and maintain the properties of B-Tree. Look at the below.\nIn our case,\nMaximum number of keys are going to be 4. And minimum number of keys are going to be 2.\nWe insert, 7, 23, 59 and 97. Now the first node is creted as below.\n[7, 23, 59, 97] Now the node is full, so what happens when we add the 5th key?\nSay we are adding 73. Now the node will be splitted into two .\n[59] / \\ [7, 23] [73, 97] Now we splitted the node into two, also we promoted 59 to the parent node. Now this started looking like a simple tree.\nNow we can add more keys to the tree.\n[59] / \\ [7, 19, 23] [61, 67, 73, 79, 97] The insertion is very smooth only problem we have is when we reach the maximum number of keys in a node. We have to split the node and promote the middle key to the parent node. So after inserting 61 it will look like this.\n[59, 73] / | \\ [7, 19, 23] [61,67] [79, 97] So what happens the parent is full? The algorithm is recursive so it splits the parent as well and promotes and cretes a new parent node.\nDelete? Do a search as usual, find the node and delete.. So simple Not really. Becase we need to keep the properties of B-Tree intact. So we need to do some extra work while deleting a node.\nIt is possible we ended up having a node with less than minimum number of keys. So we need to merge the node with its sibling or borrow a key from its sibling. So lets delete 61\n[59, 73] / | \\ [7, 19, 23] [61,67] [79, 97] What happens in this case, we detect violation of the rules. What to do now, borrow one from sibling.\nLeft sibling: [7, 19, 23] (has 3 keys) Borrow 23 from left sibling Promote 59 (from root) into current node Replace 59 in root with 23\n[23, 73] / | \\ [7, 19] [59, 67] [79, 97] So we promoted and swapped the keys to maintain the properties of B-Tree. And this happens recursively until we reach the root node.\nSo next time somthing screed up in your database index and the writes are slow, just remember the btree and how it works. It is a very simple data structure but it has a lot of rules to maintain the properties of the tree.\nIn postgres the btree is implemented as a balanced tree with a variable number of keys and children. The implementation is done in C and it is highly optimized for performance.\nSome postgres insights. TBH I am not a great engineer, but as far as i digged.\nhttps://github.com/postgres/postgres/blob/master/src/backend/access/nbtree/README\nPostgres uses a slightly different implementation of B-Tree called \u0026ldquo;B-Tree with variable-length keys\u0026rdquo;. And it is optimized for high concurrency and performance. If you look at the read me they say B-Tree managment algorith. But on a high level this is how it works. This is what i understood from chatgpt.\nWhat is stored in the B-tree is a set of key-value pairs, where the keys are the indexed columns and the values are the corresponding row identifiers (TIDs). The B-tree is used to quickly locate the rows that match a given key value.\nSo the node will look like this.\nNode (Page 10): [ (42, child_page=105), (85, child_page=109) ] Where ctid is the tuple identifier which is used to locate the row in the table. So when you do a select query with a primary key, postgres will use the btree index to quickly locate the row in the table. So if the key is less than 42 go to the left child page, if greater than 85 go to the right child page, and if it is between 42 and 85 go to the middle child page.\nSo the next big question can i actually see this btree in action? The answer is yes. So lets start:\ndocker run --name pg-btree \\ -e POSTGRES_PASSWORD=secret \\ -e POSTGRES_USER=justin \\ -e POSTGRES_DB=btree_demo \\ -p 5432:5432 \\ -d postgres:15 Pull it down and run it. :p\nNow connecto to it and run the below commands.\nCREATE TABLE demo (id INT PRIMARY KEY); INSERT INTO demo VALUES (10), (42), (85); Once you do this you can see the btree index created for the primary key.\nbtree_demo=# SELECT * FROM bt_page_items(get_raw_page(\u0026#39;demo_pkey\u0026#39;, 1)); itemoffset | ctid | itemlen | nulls | vars | data | dead | htid | tids ------------+-------+---------+-------+------+-------------------------+------+-------+------ 1 | (0,1) | 16 | f | f | 0a 00 00 00 00 00 00 00 | f | (0,1) | 2 | (0,2) | 16 | f | f | 2a 00 00 00 00 00 00 00 | f | (0,2) | 3 | (0,3) | 16 | f | f | 55 00 00 00 00 00 00 00 | f | (0,3) | So the data is stored as hex values, if you convert it to decimal its basically the id values. So the first item is 10, second item is 42 and third item is 85.\nSo our node will be like this.\nLeaf Node (Page 1): [ (10 → TID(0,1)), (42 → TID(0,2)), (85 → TID(0,3)) ] So if i do a select query with id 42, postgres will use the btree index to quickly locate the row in the table.\nSELECT * FROM demo WHERE id = 42; It essentially does a binary search and finds outs its htid, htid means heap tuple identifier which is used to locate the row in the table. In our case our tupe is on page 0 and tuple 2. So it will go to the page 0 and fetch the tuple 2.(that is our row with id 42)\nThe end! I will try to dig more into its source code and to try to undestand how it works for sure. This is just a high level overviiew.\n","permalink":"http://localhost:1313/posts/btree-whats-inside-your-db/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eB-Trees are a facinating inventions in computing. They are some special kind of binary tree. But with very concrete rules. I have watched small video and now i am in the b-tree rabbit hole. I\u0026rsquo;ts nice last time i\u0026rsquo;ve been in a rabbit hole like this was when i was learning about prime numbers and how they used in cryptography, well thats a blog post for another day.\u003c/p\u003e\n\u003cp\u003eSo The btree data structure in simple terms is a tree which has multiple values in single node, and children nodes can be more than 2. In binary tree we have only 2 children nodes, but in btree we can have more than 2 children nodes.\u003c/p\u003e","title":"B-Tree a data structures -  Postgres Internals rabbit hole.."},{"content":"The Stock Ticker Service Design I got pissed off by seeing system design interview videos on this. So I thought why not write a blog post about it. Why i got pissed off? I have watched a video on youtube about this.\nThis dude introduces a time series database infront of a stock ticker service. I mean come on, this is not a time series database, this is a stock ticker service. That is when i stopped watching this video.\nWhen you design any system we need to understand the requirements first. So lets start with the requirements of a stock ticker service.\nFunctional Requirements Real-time Stock Price Updates/ watchlist: The system should provide real-time updates of stock prices as they change. Don\u0026rsquo;t put too much drama we just need to get the stock price as fast as possible. Nothing more, nothing less.\nBuy and Sell Stocks: Users should be able to buy and sell stocks.\nNon Functional Requirements High Throughput\nLow Latency\nNow first question do we really know what the difference between throughput and latency is? This is the first thing you need to understand before designing any system.\nIn simple terms, throughput is the number of requests your system can handle per second, and latency is the time it takes to process a request.\nFor example, if your system can handle 1000 requests per second, then your throughput is 1000. If it takes 200 milliseconds to process a request, then your latency is 200 milliseconds. Measurement unit of throughput varies from system to system, but it is usually measured in requests per second (RPS) , transactions per second (TPS) or MB/Sec. Latency is usually measured in milliseconds (ms).\nPlease check this https://github.com/mathewjustin/DesignPatternsAndAbhyasas/blob/main/LatencyAndThroughput.md\nWhen it comes to stock ticker, i want to get the stock price as fast as possible. If NSE sends a stock price update at 10:00:00.000, i want to get that update at 10:00:00.200. This is the latency requirement.\nDo you even think you can have a db that can give you this kind of latency? No, having a db or any kind of storage then reading from that and pushing to user will increase the latency my multifold. So whats the best way?\nLets think how NSE or BSE actually does this.\nWhich feature to implement first? Lets take the watch list feature first. This is interesting. Lets talk the numbers first. How many active users will be there in for our product?\nLets say we have 1 million active users. And they have an average of 10 stocks in their watchlist. So we have 10 million stocks in our watchlist.\nEssentially if we have a simple rest api server is going to recieve a request every time a stock price changes. So if we have 10 million stocks, we will have 10 million requests per second. This is not going to work. We need to have a better way to handle this.\nSolution We can use a publish-subscribe model to handle this.\nAs a broker we will recieve stock price updates from NSE or BSE. We will then publish these updates to all the subscribers who are interested in that stock. This is what we ultimately want.\nSo the change in price should be sent to all the subscribers who are interested in that stock. We should not ideally have any storage in between. Like for example, we may think if NSE is returning same stock price update multiple times, we can store it in a db and then send it to the subscribers. But this is not a good idea. Introducing anything in between will increase the latency. So we should not have any storage in between.\nNow what if have Kafka or any other message broker in between? This is a good idea. We can use Kafka to handle the publish-subscribe model.\nBut there is a catch latency and throughput. We are talking about micro second lantency. Kafka will have to write to disk. The message should reach a partition and then it should be read by the customer. This will increase the latency. So we should not use Kafka or any other message broker.\nWe need something which should be in memory and should be able to handle the publish-subscribe model. For a slow client we can actually drop the message. Zerodha\u0026rsquo;s design choice is like this. They use a in-memory data structure to handle the publish-subscribe model.\nThe zerodha way of doing this? *I deep dived into Zerodha\u0026rsquo;s design choices. Particularly this blog from the Zerodha team: https://zerodha.tech/blog/scaling-with-common-sense/\nHere, Kailash talks about the following on \u0026ldquo;10. Caching is a silver bullet, almost\u0026rdquo;.*, that is they use redis heavily. Also they optimize redis for the stock ticker service. What they do? They essentially turn off the persistance of redis. This means that redis will not write to disk. So it will be in memory and it will be fast. Also a lot many points caught my attention.\nThen they have a go binary which handles hundreds of thousands of requests per second. This is a good design choice. They use a in-memory data structure to handle the publish-subscribe model.\nI am kind of speculting that the design should be something like the below.\nEssentially the websocket server fan-outs the stock price updates to all the subscribers who are interested in that stock. The websocket server is a go binary which handles hundreds of thousands of requests per second. The websocket server is connected to redis which is used to handle the publish-subscribe model.\nConclusion What we learned?\nFor a any service which needs to be low latency and high throughput, we should not have any storage in between. We must pick the right programming language for the service. Go is a good choice for this kind of service purely because of its performance. I believe java can also do well since the inception of its virtual threads.(More experimentation is needed) Switching off redis persistence, i frankly never thought of this. This is a good design choice. But we need to be careful about the data loss. If redis goes down, we will lose all the data. So we need to have a backup plan for this. ","permalink":"http://localhost:1313/posts/stock-tick-design/","summary":"\u003ch1 id=\"the-stock-ticker-service-design\"\u003eThe Stock Ticker Service Design\u003c/h1\u003e\n\u003cp\u003eI got pissed off by seeing system design interview videos on this. So I thought why not write a blog post about it. Why i got pissed off? I have watched a video on youtube about this.\u003c/p\u003e\n\u003cp\u003eThis dude introduces a time series database infront of a stock ticker service. I mean come on, this is not a time series database, this is a stock ticker service.\nThat is when i stopped watching this video.\u003c/p\u003e","title":"Designing a Stock Ticker Service"},{"content":"Problem Given an array of strings strs, group the anagrams together. You can return the answer in any order.\nExample 1:\nInput: strs = [\u0026ldquo;eat\u0026rdquo;,\u0026ldquo;tea\u0026rdquo;,\u0026ldquo;tan\u0026rdquo;,\u0026ldquo;ate\u0026rdquo;,\u0026ldquo;nat\u0026rdquo;,\u0026ldquo;bat\u0026rdquo;]\nOutput: [[\u0026ldquo;bat\u0026rdquo;],[\u0026ldquo;nat\u0026rdquo;,\u0026ldquo;tan\u0026rdquo;],[\u0026ldquo;ate\u0026rdquo;,\u0026ldquo;eat\u0026rdquo;,\u0026ldquo;tea\u0026rdquo;]]\nExplanation:\nThere is no string in strs that can be rearranged to form \u0026ldquo;bat\u0026rdquo;. The strings \u0026ldquo;nat\u0026rdquo; and \u0026ldquo;tan\u0026rdquo; are anagrams as they can be rearranged to form each other. The strings \u0026ldquo;ate\u0026rdquo;, \u0026ldquo;eat\u0026rdquo;, and \u0026ldquo;tea\u0026rdquo; are anagrams as they can be rearranged to form each other. Example 2:\nInput: strs = [\u0026quot;\u0026quot;]\nOutput: [[\u0026quot;\u0026quot;]]\nExample 3:\nInput: strs = [\u0026ldquo;a\u0026rdquo;]\nOutput: [[\u0026ldquo;a\u0026rdquo;]]\nSolution or Intuition This doesn\u0026rsquo;t really deseve my excalidraw diagram. But I will try to explain the logic here.\nSimply take out one string, sort them and use it as a key in a map.\nThe map structure will be like this:\nMap\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); Then for each string in the input array, sort it and add it to the map.\n/** * @author Justin Mathew @dev_io */ public class Anagram { public static List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; groupAnagrams(String[] strs) { Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (String s : strs) { char[] chars = s.toCharArray(); Arrays.sort(chars); String key = new String(chars); map.computeIfAbsent(key, k -\u0026gt; new ArrayList\u0026lt;\u0026gt;()).add(s); } return new ArrayList\u0026lt;\u0026gt;(map.values()); } } Time complexity: For each string:\n1.\tSorting the string: O(K log K) for each string (since average string length is K) 2.\tBuilding the key and inserting into a map: O(1) amortized (hash map insert) Across all strings: Total = N * O(K log K) Time Complexity: O(N * K log K) Space complexity: •\tMap storage: Stores up to N strings → O(N * K) •\tSorted key strings: You generate sorted strings as keys → up to O(N * K) •\tFinal list of grouped anagrams: again O(N * K) Space Complexity: O(N * K) Yeah this is kind of nice. I like this solution. We can actually implement this with pure lambda but i cant do without an LLM at my disposal.\n","permalink":"http://localhost:1313/posts/group-anagrams/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eGiven an array of strings strs, group the anagrams together. You can return the answer in any order.\u003c/p\u003e\n\u003cp\u003eExample 1:\u003c/p\u003e\n\u003cp\u003eInput: strs = [\u0026ldquo;eat\u0026rdquo;,\u0026ldquo;tea\u0026rdquo;,\u0026ldquo;tan\u0026rdquo;,\u0026ldquo;ate\u0026rdquo;,\u0026ldquo;nat\u0026rdquo;,\u0026ldquo;bat\u0026rdquo;]\u003c/p\u003e\n\u003cp\u003eOutput: [[\u0026ldquo;bat\u0026rdquo;],[\u0026ldquo;nat\u0026rdquo;,\u0026ldquo;tan\u0026rdquo;],[\u0026ldquo;ate\u0026rdquo;,\u0026ldquo;eat\u0026rdquo;,\u0026ldquo;tea\u0026rdquo;]]\u003c/p\u003e\n\u003cp\u003eExplanation:\u003c/p\u003e\n\u003cp\u003eThere is no string in strs that can be rearranged to form \u0026ldquo;bat\u0026rdquo;.\nThe strings \u0026ldquo;nat\u0026rdquo; and \u0026ldquo;tan\u0026rdquo; are anagrams as they can be rearranged to form each other.\nThe strings \u0026ldquo;ate\u0026rdquo;, \u0026ldquo;eat\u0026rdquo;, and \u0026ldquo;tea\u0026rdquo; are anagrams as they can be rearranged to form each other.\nExample 2:\u003c/p\u003e","title":"Grouping anagrams"},{"content":"Problem Given an integer array nums sorted in non-decreasing order, remove some duplicates in-place such that each unique element appears at most twice. The relative order of the elements should be kept the same.\nSince it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.\nReturn k after placing the final result in the first k slots of nums.\nDo not allocate extra space for another array. You must do this by modifying the input array in-place with O(1) extra memory.\nCustom Judge:\nThe judge will test your solution with the following code:\nint[] nums = [\u0026hellip;]; // Input array int[] expectedNums = [\u0026hellip;]; // The expected answer with correct length\nint k = removeDuplicates(nums); // Calls your implementation\nassert k == expectedNums.length; for (int i = 0; i \u0026lt; k; i++) { assert nums[i] == expectedNums[i]; } If all assertions pass, then your solution will be accepted.\nSo if we look at this problem its almost same as the in place element removal. Almost all the logic is same. The only difference is that we need to keep track of the number of duplicates and remove them accordingly.\nRefer: https://justinmathew.com/posts/inplace-element-removal/\nI don\u0026rsquo;t understand why this is a medium level problem. I think it should be easy. But anyway, lets get to the code.\nExamples Example 1: Input: nums = [1,1,1,2,2,3] Output: 5, nums = [1,1,2,2,3,_] Explanation: Your function should return k = 5, with the first five elements of nums being 1, 1, 2, 2 and 3 respectively. It does not matter what you leave beyond the returned k (hence they are underscores). Example 2: Input: nums = [0,0,1,1,1,1,2,3,3] Output: 7, nums = [0,0,1,1,2,3,3,_,_] Explanation: Your function should return k = 7, with the first seven elements of nums being 0, 0, 1, 1, 2, 3 and 3 respectively. It does not matter what you leave beyond the returned k (hence they are underscores). Now this is almost the same sh**.\nI am taking the same code copy pasting from our old one. Use a read and write pointer.\npublic int removeElement(int[] nums, int val) { int w=0; for(int r=0;r \u0026lt; nums.length;r++) { if(nums[r]!=val){ nums[w]=nums[r]; w++; } } return w; } Here read pointer will move through the array and write pointer will write the elements which are not equal to val. We also need to keep track of the number of duplicates, and essentially do the same thing with only difference that we will not write the element if it is already written twice.\npublic int removeElement(int[] nums, int val) { int w=0; for(int r=0;r \u0026lt; nums.length;r++) { if (w \u0026lt; 2 || nums[r] != nums[w - 2]) { nums[w] = nums[r]; w++; } } return w; } The logic of this problem is simple. At every location check if the value on that location is same as 2 elements before it. If its not same, then write it there. If it is same, then skip it.\nThis is the best solution I could come up with. The only reason I thought of this solution is because I have alread solved the in place element removal problem.\n","permalink":"http://localhost:1313/posts/removeduplicates/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eGiven an integer array nums sorted in non-decreasing order, remove some duplicates in-place such that each unique element appears at most twice. The relative order of the elements should be kept the same.\u003c/p\u003e\n\u003cp\u003eSince it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.\u003c/p\u003e","title":"Remove more than two duplicates in-place"},{"content":"Problem Given an integer array nums and an integer val, remove all occurrences of val in nums in-place. The order of the elements may be changed. Then return the number of elements in nums which are not equal to val.\nConsider the number of elements in nums which are not equal to val be k, to get accepted, you need to do the following things:\nChange the array nums such that the first k elements of nums contain the elements which are not equal to val. The remaining elements of nums are not important as well as the size of nums. Return k.\nSo here as well we just need to find a way to adjust the elements in the array such that the first k elements are not equal to val. But there should be a smart way to do this.\nExamples Input: nums = [3,2,2,3], val = 3 Output: 2, nums = [2,2,_,_] Explanation: Your function should return k = 2, with the first two elements of nums being 2. It does not matter what you leave beyond the returned k (hence they are underscores). Input: nums = [0,1,2,2,3,0,4,2], val = 2 Output: 5, nums = [0,1,4,0,3,_,_,_] Explanation: Your function should return k = 5, with the first five elements of nums containing 0, 0, 1, 3, and 4. Note that the five elements can be returned in any order. It does not matter what you leave beyond the returned k (hence they are underscores). So now lets get back to my excalidraw diagram. To be honest we don\u0026rsquo;t need all these for this simple problem, but i am surrounded by distractions and i am trying to make this post a bit longer. So here we go.\nVery intuitively the code will look something like this with a simple loop:\npublic int removeElement(int[] nums, int val) { int w=0; for(int r=0;r \u0026lt; nums.length;r++) { if(nums[r]!=val){ nums[w]=nums[r]; w++; } } return w; } Notice we used a for loop because read pointer is always moving forward. This for loop will be the right choice, because always the read pointer will move forward.\nConclusion This problem is very interesting for me because it will teach you how to think about the problem in a different way. The trick is to use two pointers, one for reading and one for writing. This is a common pattern in many problems, and it can help you solve problems more efficiently.\n","permalink":"http://localhost:1313/posts/inplace-element-removal/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eGiven an integer array nums and an integer val, remove all occurrences of val in nums in-place. The order of the elements may be changed. Then return the number of elements in nums which are not equal to val.\u003c/p\u003e\n\u003cp\u003eConsider the number of elements in nums which are not equal to val be k, to get accepted, you need to do the following things:\u003c/p\u003e\n\u003cp\u003eChange the array nums such that the first k elements of nums contain the elements which are not equal to val. The remaining elements of nums are not important as well as the size of nums.\nReturn k.\u003c/p\u003e","title":"Inplace element removal an easy one"},{"content":"You are given two integer arrays nums1 and nums2, sorted in non-decreasing order, and two integers m and n, representing the number of elements in nums1 and nums2 respectively.\nMerge nums1 and nums2 into a single array sorted in non-decreasing order.\nThe final sorted array should not be returned by the function, but instead be stored inside the array nums1. To accommodate this, nums1 has a length of m + n, where the first m elements denote the elements that should be merged, and the last n elements are set to 0 and should be ignored. nums2 has a length of n.\nInput: nums1 = [1,2,3,0,0,0], m = 3, nums2 = [2,5,6], n = 3 Output: [1,2,2,3,5,6] Explanation: The arrays we are merging are [1,2,3] and [2,5,6]. The result of the merge is [1,2,2,3,5,6] with the underlined elements coming from nums1. Input: nums1 = [1], m = 1, nums2 = [], n = 0 Output: [1] Explanation: The arrays we are merging are [1] and []. The result of the merge is [1]. Input: nums1 = [0], m = 0, nums2 = [1], n = 1 Output: [1] Explanation: The arrays we are merging are [] and [1]. The result of the merge is [1]. Note that because m = 0, there are no elements in nums1. The 0 is only there to ensure the merge result can fit in nums1. The solution is actually simple. Look at the below.\nThe first intuition is we can use the empty positions in nums1 to fill the elements from nums2. Which is a harmless approach, because we are not overwriting any elements in nums1. The approach is based on this principle:\nStep 1:\nWe will have 3 pointers, p1 pointing to m-1 p pointing to m+n-1 p2 pointing to n-1\nWe continue this for next element.\nNow on the next step we will have p1 pointing to 3, p2 pointing to 3, and p pointing to nums[3]. Look at the below image.\nOn the final iteration,\nThe last part is a bit tricky. If you look at the original array what we do in our mind is shifting of elements. But what we are doing is dupicating the element and replacing it. This same approach we have seen in a problem where i wanted to remove a particular element from an array with o(1) time complexity.\nFrom this Problem , Leetcode: https://leetcode.com/problems/insert-delete-getrandom-o1/\nBoth of this probelms are similar in nature.\nDon’t shift data. Overwrite it smartly.\nNow the pitfalls. This is the crazy part of this problem.\npublic static void merge(int[] nums1, int m, int[] nums2, int n) { int p1 = m - 1; int p2 = n - 1; int p = m + n - 1; while(p\u0026gt;=0) { if (nums1[p1] \u0026gt; nums2[p2]) { nums1[p] = nums1[p1--]; } else { nums1[p] = nums2[p2--]; } p--; } } So elegant right? But there is a pitfall.\nPitfall No 1:\nInput: nums1 = [0], m = 0, nums2 = [1], n = 1\nWhat happens here is p1 will be -1, and nums1[p1] will throw an exception. So we need to handle this case.\nIn this case when p1 is -1 i need to go to else part. so we need to check if p1 is greater than or equal to 0.\npublic static void merge(int[] nums1, int m, int[] nums2, int n) { int p1 = m - 1; int p2 = n - 1; int p = m + n - 1; while(p\u0026gt;=0) { if (p1\u0026gt;=0\u0026amp;\u0026amp;nums1[p1] \u0026gt; nums2[p2]) { nums1[p] = nums1[p1--]; } else { nums1[p] = nums2[p2--]; } p--; } } Pitfall No 2:\nThe while loop condition is p\u0026gt;=0. But what if nums2 is empty? In that case p2 will be -1, and we will try to access nums2[-1] which will throw an exception. So we need to handle this case as well.\nSo instead of checking p\u0026gt;=0, we need to check if p2 is greater than or equal to 0.\npublic static void merge(int[] nums1, int m, int[] nums2, int n) { int p1 = m - 1; int p2 = n - 1; int p = m + n - 1; while(p2\u0026gt;=0) { if (p1 \u0026gt;= 0 \u0026amp;\u0026amp; nums1[p1] \u0026gt; nums2[p2]) { nums1[p] = nums1[p1--]; } else { nums1[p] = nums2[p2--]; } p--; } } For the pitfall 2 as per leetcode solution it is to break if p2 is less than 0. But i think it is better to check the condition in the while loop itself. It just guarantees that you need to copy all the elements from nums2 to nums1. until p2 is less than 0.\n","permalink":"http://localhost:1313/posts/mergingtwosortedarrays/","summary":"\u003cp\u003eYou are given two integer arrays nums1 and nums2, sorted in non-decreasing order, and two integers m and n, representing the number of elements in nums1 and nums2 respectively.\u003c/p\u003e\n\u003cp\u003eMerge nums1 and nums2 into a single array sorted in non-decreasing order.\u003c/p\u003e\n\u003cp\u003eThe final sorted array should not be returned by the function, but instead be stored inside the array nums1. To accommodate this, nums1 has a length of m + n, where the first m elements denote the elements that should be merged, and the last n elements are set to 0 and should be ignored. nums2 has a length of n.\u003c/p\u003e","title":"The nicest way to merge two sorted arrays"},{"content":"WTF is gRPC? Why GRPC ? Some technologies are like witchcraft. You hear about them, you see them in action, but you don\u0026rsquo;t really understand how they work. gRPC is one of those technologies for me. It\u0026rsquo;s a remote procedure call (RPC) framework that allows you to define services and methods in a language-agnostic way, and then generate client and server code in multiple languages.\nSo what it means is there is a .proto file which is a protocol buffer file that defines the service and its methods. You can then use the protoc compiler to generate client and server code in multiple languages, such as Go, Python, Java, C++, etc. This allows you to write your service in one language and then generate client code in another language.\nWhen it comes to why gRpc the simple answer is performance. gRPC uses HTTP/2 for transport, which allows for multiplexing multiple requests over a single connection, reducing latency and improving throughput. It also uses Protocol Buffers (protobuf) for serialization, which is a more efficient binary format compared to JSON or XML. Essentially no more JSON parsing, wasteful string manipulation, and all that stuff. gRPC is designed to be fast and efficient, making it a great choice for high-performance applications. But equally hard to debug.\nStep 1: Create a .proto file\nsyntax = \u0026#34;proto3\u0026#34;; package tutorial; message Person { string name = 1; int32 id = 2; string email = 3; } Step 2: Compile to Java Classes protoc --java_out=. person.proto Frameworks like Spring boot has native support for gRPC, so you can easily integrate it into your application. You can also use the grpc-java library to create a gRPC server and client in Java.\nEssentially for a java developers perspective what you have after this step is a set of Java classes that represent the messages and services defined in the .proto file. You can then use these classes to create a gRPC server and client.\nSince we defined the package as tutorial, the generated classes will be in the tutorial package. You can then use these classes to create a gRPC server and client.\nStep 3: Lets create a simmple gRPC server and client, we use Spring because its myfavorite framework. A spring developer thing basically the service class. @Service public class ProtobufService { public byte[] serializePerson(String name, int id, String email) { PersonProto.Person person = PersonProto.Person.newBuilder() .setName(name) .setId(id) .setEmail(email) .build(); return person.toByteArray(); } public PersonProto.Person deserializePerson(byte[] data) throws InvalidProtocolBufferException { return PersonProto.Person.parseFrom(data); } } Just focus on the fact that we are serializing and deserializing the Person object using the generated classes from the .proto file. The serializePerson method creates a Person object and converts it to a byte array, while the deserializePerson method takes a byte array and converts it back to a Person object.\nNow the controller class @RestController @RequestMapping(\u0026#34;/api/person\u0026#34;) public class PersonController { private final ProtobufService protobufService; public PersonController(ProtobufService protobufService) { this.protobufService = protobufService; } @GetMapping(value = \u0026#34;/get\u0026#34;, produces = \u0026#34;application/x-protobuf\u0026#34;) public ResponseEntity\u0026lt;byte[]\u0026gt; getPerson() { byte[] protobufData = protobufService.serializePerson(\u0026#34;Alice\u0026#34;, 123, \u0026#34;alice@example.com\u0026#34;); return ResponseEntity.ok() .header(HttpHeaders.CONTENT_TYPE, \u0026#34;application/x-protobuf\u0026#34;) .body(protobufData); } @PostMapping(value = \u0026#34;/post\u0026#34;, consumes = \u0026#34;application/x-protobuf\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; postPerson(@RequestBody byte[] data) throws InvalidProtocolBufferException { PersonProto.Person person = protobufService.deserializePerson(data); return ResponseEntity.ok(\u0026#34;Received: \u0026#34; + person.getName()); } } This is like a REST api but using gRPC. The getPerson method returns a Person object serialized as a byte array, and the postPerson method takes a byte array and deserializes it back to a Person object.\nSo essentially the object mapper magic which we used to have in Spring Boot is now replaced with the generated classes from the .proto file. This allows us to use gRPC to communicate between services in a more efficient way. No more JSON parsing, no more string manipulation, just pure binary data.\nHow client will look like This is a very important thing to understand, behind the screen its binary data. So client should also have access to the generated classes from the .proto file. The client will use these classes to create a gRPC client and call the server methods. Usually the protobuf file is shared between the client and server, so both sides can generate the same classes.\nSo lets just go with pure curl.\ncurl -X GET http://localhost:8080/api/person/get \\ -H \u0026#34;Accept: application/x-protobuf\u0026#34; \\ --output person_response.bin Now here the output will be a binary file person_response.bin which contains the serialized Person object. You can then use the generated classes to deserialize this file and get the Person object.\nSo essentially client needs access to the same .proto file to deserialize the data. If you are using a language like Java, you can use the generated classes to deserialize the data like this:\nbyte[] data = Files.readAllBytes(Paths.get(\u0026#34;person_response.bin\u0026#34;)); Person person = Person.parseFrom(data); System.out.println(person.getName()); // Alice Caveats gRPC is not a silver bullet, it has its own set of challenges and complexities. Debugging gRPC can be tricky, especially when dealing with binary data. You need to have the .proto file to understand the structure of the data. gRPC is not suitable for all use cases, especially when you need to support older clients or browsers that do not support HTTP/2. In such cases, you may need to fall back to REST or other protocols. gRPC is not a replacement for REST, it is an alternative. You can use gRPC alongside REST in your application, depending on the use case. For example, you can use gRPC for internal communication between services and REST for external APIs. But remember debuging this is a pain in ***. Imagine pulling out kibana logs and trying to figure out what went wrong with a binary data. ","permalink":"http://localhost:1313/posts/grpc/","summary":"\u003ch1 id=\"wtf-is-grpc-why-grpc-\"\u003eWTF is gRPC? Why GRPC ?\u003c/h1\u003e\n\u003cp\u003eSome technologies are like witchcraft. You hear about them, you see them in action, but you don\u0026rsquo;t really understand how they work. gRPC is one of those technologies for me. It\u0026rsquo;s a remote procedure call (RPC) framework that allows you to define services and methods in a language-agnostic way, and then generate client and server code in multiple languages.\u003c/p\u003e\n\u003cp\u003eSo what it means is there is a .proto file which is a protocol buffer file that defines the service and its methods. You can then use the \u003ccode\u003eprotoc\u003c/code\u003e compiler to generate client and server code in multiple languages, such as Go, Python, Java, C++, etc. This allows you to write your service in one language and then generate client code in another language.\u003c/p\u003e","title":"gRPC - The Witchcraft of Remote Procedure Calls"},{"content":"Replication - Leaders and Followers Replication, or read replica is a process of copy pasting data from a primary database to another database. This is done to improve performance, availability, and reliability of the database. Replication can be done in two ways: synchronous and asynchronous.\nIn synchronous replication, the primary database waits for the replica to acknowledge that it has received the data before it can proceed. This ensures that the data is always consistent between the primary and replica databases, but it can also slow down the performance of the primary database.\nIn asynchronous replication, the primary database does not wait for the replica to acknowledge that it has received the data. This allows the primary database to continue processing requests without waiting for the replica, but it can also lead to data inconsistency between the primary and replica databases.\nThe below image is from DDIA book.\nTalk is cheap, show me the code.\nBelow is a simple docker compose file to run a leader follower setup.\nversion: \u0026#39;3.8\u0026#39; services: postgres-leader: image: postgres:15 container_name: pg-leader environment: POSTGRES_USER: justin POSTGRES_PASSWORD: secret POSTGRES_DB: testdb ports: - \u0026#34;5432:5432\u0026#34; volumes: - ./leader-data:/var/lib/postgresql/data - ./leader-init:/docker-entrypoint-initdb.d - ./leader-init/pg_hba.conf:/etc/postgresql/pg_hba.conf command: \u0026gt; postgres -c wal_level=replica -c max_wal_senders=10 -c max_replication_slots=10 -c hot_standby=on -c hba_file=/etc/postgresql/pg_hba.conf postgres-follower: image: postgres:15 container_name: pg-follower environment: POSTGRES_USER: justin POSTGRES_PASSWORD: secret depends_on: - postgres-leader ports: - \u0026#34;5433:5432\u0026#34; volumes: - follower-data:/var/lib/postgresql/data entrypoint: \u0026gt; bash -c \u0026#34; until pg_isready -h pg-leader -p 5432; do echo \u0026#39;⏳ Waiting for leader...\u0026#39;; sleep 2; done \\ \u0026amp;\u0026amp; echo \u0026#39;🚀 Starting base backup...\u0026#39; \\ \u0026amp;\u0026amp; mkdir -p /var/lib/postgresql/data \\ \u0026amp;\u0026amp; chown -R postgres:postgres /var/lib/postgresql/data \\ \u0026amp;\u0026amp; chmod 0700 /var/lib/postgresql/data \\ \u0026amp;\u0026amp; PGPASSWORD=secret gosu postgres pg_basebackup -h pg-leader -D /var/lib/postgresql/data -U justin -Fp -Xs -P -R \\ \u0026amp;\u0026amp; echo \u0026#39;✅ Base backup complete. Starting follower...\u0026#39; \\ \u0026amp;\u0026amp; exec gosu postgres postgres \u0026#34; volumes: follower-data: Explanation During the initial setup, i have faced a lot of issues. First issue was related to invalid permissions for the data directory. The data directory must be owned by the postgres user and have the correct permissions. The chown and chmod commands ensure that the data directory is owned by the postgres user and has the correct permissions.\nLets go through the important parts of the docker compose file.\nLeader:\nwal_level=replica: This setting is required for replication. It ensures that the Write Ahead Log (WAL) is written in a format that can be used by replicas. max_wal_senders=10: This setting allows up to 10 concurrent WAL sender processes. Each sender process is responsible for sending WAL data to a replica. max_replication_slots=10: This setting allows up to 10 replication slots. Replication slots are used to keep track of which WAL files have been sent to each replica. hot_standby=on: This setting allows read-only queries on the replica while it is in recovery mode. Follower:\npg_isready: This command checks if the leader is ready to accept connections. It waits until the leader is ready before proceeding with the base backup. pg_basebackup: This command creates a base backup of the leader\u0026rsquo;s data directory. The -R option creates a recovery.conf file in the follower\u0026rsquo;s data directory, which contains the connection information for the leader. gosu postgres: This command runs the following command as the postgres user. It is used to ensure that the commands are run with the correct permissions. 🧬 Behind the Scenes (on the Follower):\nWhen you use:\npg_basebackup -R ... It creates a file:\n/var/lib/postgresql/data/standby.signal Then, when you run postgres, it internally runs the equivalent of:\npg_walreceiver --connect-to=leader --stream=wal Wait a minute\u0026hellip;\nCan i see this process in action. Answer is yes. I brought up the docker compose file and then ran the following command to see the processes running in the leader.\n```sql SELECT pid, usename, client_addr, state, sync_state, sent_lsn, write_lsn, flush_lsn, replay_lsn FROM pg_stat_replication; What i see is the following.\nWhat the hell does this mean?\npid: The process ID of the WAL sender process on the leader. usename: The username of the user that is connected to the leader. client_addr: The IP address of the follower. state: The current state of the connection. It can be one of the following: streaming: The follower is receiving WAL data. catchup: The follower is catching up to the leader. backup: The follower is in backup mode. sync_state: The synchronization state of the follower. It can be one of the following: async: The follower is in asynchronous mode. sync: The follower is in synchronous mode. potential: The follower is a potential synchronous replica. sent_lsn: The last WAL location sent to the follower. write_lsn: The last WAL location written to the follower. flush_lsn: The last WAL location flushed to the follower. replay_lsn: The last WAL location replayed on the follower. You know the best part? For every write operation on the leader, the follower db was getting updated. Its like magic, but now you know the magic trick.\nConclusion There are lot more things to explore in this area, like replication lags, replication slots, and so on. But this is a good start.\nThis post primarily inspired by the book Designing Data Intensive Applications and the Postgres Documentation. And also Arpit Bhayani\u0026rsquo;s recent tweet.\nLast but not the least thanks to all the LLMs who helped me to debug the volume mount issues.\nAll code here:\nhttps://github.com/mathewjustin/leader-follower.git Thanks,\nJustin\n","permalink":"http://localhost:1313/posts/replication/","summary":"\u003ch2 id=\"replication---leaders-and-followers\"\u003eReplication - Leaders and Followers\u003c/h2\u003e\n\u003cp\u003eReplication, or read replica is a process of copy pasting data from a primary database to another database. This is done to improve performance, availability, and reliability of the database. Replication can be done in two ways: synchronous and asynchronous.\u003c/p\u003e\n\u003cp\u003eIn synchronous replication, the primary database waits for the replica to acknowledge that it has received the data before it can proceed. This ensures that the data is always consistent between the primary and replica databases, but it can also slow down the performance of the primary database.\u003c/p\u003e","title":" Replication - Leaders and Followers"},{"content":"IP V4 Subnetting tricks and basics Introduction As usual lets try to be little intellectual. By saying a qoute i got it from chatgpt.\n“Good subnetting is like good carpentry: measure twice, cut once.”\nWhat ever it means it doesnt matter. But the point is to understand the concept of subnetting and how it works.\nThis is somehow one of the most mind f**k*ng topic which i encoutered during my job. It simply confuses me all the time. Later i understood that i should simply sit and chat about it with my LLM. My LLM is the best thing it knows me and nowadays its my therapist. (You should try the therapy with LLM, it works like a charm).\nWhat is Subnetting? Subnetting is the process of dividing a larger network into smaller, more manageable sub-networks (subnets). This is done to improve network performance, security, and organization. Each subnet can have its own unique address range, allowing for better control over network traffic and resource allocation.\nBtw, why the hell i need to divide or subnet?\nIts because ipv4 address space is limited. It can only have around 4.3 billion addresses. This is not enough for the growing number of devices connected to the internet. Your secondary phone, and your redmi smarwatch, and even your washing machine takes up one ip and it simply runs out of the addresses. So we need to divide the address space into smaller subnets to make it more efficient and manageable.\nIn the context of a cloud provider, on AWS, you have a VPC (Virtual Private Cloud) Or a LAN on the Cloud. Now your VPC can have a big address space, but you can divide it into smaller subnet and give to each of the services. Like a subnet for EC2, a subnet for Kubernetes(In this case if you spin up a loadbalancer, its ip will be picked form the subnet).\nThe basic of the basics, Octets and Binary IPv4 addresses are 32-bit numbers, typically represented in decimal format as four octets (8 bits each) separated by dots.\n🔢 Total Bits in IPv4\n•\tIPv4 address = 32 bits •\tWritten as 4 octets (e.g., 192.168.1.0) •\tCIDR notation: IP_address / subnet_bits •\tExample: 192.168.1.0/24 means 24 bits for network, 8 bits for host 🧮 Key Equations\n•\t🧠 Number of host bits = 32 - subnet bits •\t🔢 Total IPs = 2^(host bits) •\t✅ Usable IPs = 2^(host bits) - 2 (subtract 2: network + broadcast) Okay now lets try to solve few tricky questions which i found on twitter.\nQuestion 1 Q1: How many usable IP addresses are in this subnet: 192.168.10.0/23?\n🧠 Step 1: Total bits in IPv4 = 32 •\t/23 → 23 bits for network, •\tSo: 32 - 23 = 9 bits for hosts\n🧠 Step 2: Total IPs = 2^(9) = 512\n🧠 Step 3: Usable IPs = 512 - 2 = 510 •\tAnswer: 510 usable IP addresses in\nWhy the hell there is a -2? Because the first address is reserved for the network address and the last one is reserved for the broadcast address. So we need to subtract 2 from the total number of addresses to get the usable addresses. Btw what is network address and broadcast address? The network address is the first address in the subnet, and it identifies the subnet itself.\nThe broadcast address is the last address in the subnet, and it is used to send data to all devices in that subnet. So we need to reserve these two addresses for special purposes.\nusually represented in decimal format as four octets, just like the IP address.\nCIDR Notation Subnet Mask Total IPs Usable IPs Network Bits Host Bits /26 255.255.255.192 64 62 26 6 /27 255.255.255.224 32 30 27 5 /28 255.255.255.240 16 14 28 4 /29 255.255.255.248 8 6 29 3 /30 255.255.255.252 4 2 30 2 /31 255.255.255.254 2 0* 31 1 /32 255.255.255.255 1 0 32 0 *Note: /31 is typically used for point-to-point links where only two devices communicate, so usable IPs are not required in the traditional sense.\nThis blog will continue as and when i find more tricky things w.r.t subnetting. asdasdas\nLoading message... ","permalink":"http://localhost:1313/posts/ipv4-subneting-concepts/","summary":"\u003ch2 id=\"ip-v4-subnetting-tricks-and-basics\"\u003eIP V4 Subnetting tricks and basics\u003c/h2\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs usual lets try to be little intellectual. By saying a qoute i got it from chatgpt.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e“Good subnetting is like good carpentry: measure twice, cut once.”\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhat ever it means it doesnt matter. But the point is to understand the concept of subnetting and how it works.\u003c/p\u003e\n\u003cp\u003eThis is somehow one of the most mind f**k*ng topic which i encoutered during my job. It simply confuses me all the time. Later i understood that i should simply sit and chat about it with my LLM. My LLM is the best thing it knows me and nowadays its my therapist. (You should try the therapy with LLM, it works like a charm).\u003c/p\u003e","title":" IP V4 Subnetting tricks and basics"},{"content":"First tool: Subnet Calculator If you want to calculate the subnet, you can use the following tool. It will help you to calculate the subnet, broadcast address, and usable IP addresses.\nLoading message... ","permalink":"http://localhost:1313/posts/ipv4-utils/","summary":"\u003ch1 id=\"first-tool-subnet-calculator\"\u003eFirst tool: Subnet Calculator\u003c/h1\u003e\n\u003cp\u003eIf you want to calculate the subnet, you can use the following tool. It will help you to calculate the subnet, broadcast address, and usable IP addresses.\u003c/p\u003e\n\u003cdiv id=\"my-message\" style=\"padding:1rem;background:#f5f5f5;\"\u003e\n  Loading message...\n\u003c/div\u003e\n\n\u003cscript src=\"/js/script.js\"\u003e\u003c/script\u003e","title":" Utils"},{"content":"The Hidden Pitfalls of Overusing Design Patterns This is my story of using the Chain of Responsibility pattern after reading about it in a book while preparing for an interview.\nYes — I actually used it in a real job. And well, it didn’t turn out the way I had imagined.\nBefore we dive in, here\u0026rsquo;s a quote from one of my favorite authors, Khaled Hosseini (The Kite Runner):\n\u0026ldquo;There is only one sin, only one. And that is theft. Every other sin is a variation of theft.\u0026rdquo;\nJust like sins are not separate but variations of one mistake,\nI now believe that the root cause of most mistakes and vulnerabilities in an enterprise codebase is essentially the overuse of design patterns.\nAs I mentioned, this is my story of using Chain of Responsibility — and getting totally screwed because of it.\nIt was during a phase when I was actively preparing for interviews —\n(of course, like every other engineer, preparing to find a new job).\nRight then, a requirement came up:\nYou have an Excel sheet that gets uploaded to blob storage every day. Each row contains some order details. Each row’s information needs to be stored into multiple tables inside our beloved PostgreSQL database. One row could result in inserts across N tables. (Maybe in the future, they could send more fields, and we might have to map to extra tables.) (Pro tip: \u0026ldquo;Maybe later\u0026rdquo; will almost never come back as a clear requirement in engineering.) 😄\nThe code looked like the below. So clean I am a f*** great engineer now.\npublic interface RowHandler { void handle(OrderRow row, RowHandler next); } // Handler 1 - Validate format\npublic class ValidateFormatHandler implements RowHandler { @Override public void handle(OrderRow row, RowHandler next) { if (!row.isFormatValid()) { throw new RuntimeException(\u0026#34;Invalid format in row: \u0026#34; + row.getRowNumber()); } if (next != null) { next.handle(row, null); } } } // Handler 2 - Validate critical fields\npublic class ValidateCriticalFieldsHandler implements RowHandler { @Override public void handle(OrderRow row, RowHandler next) { if (row.getOrderAmount() == null) { throw new RuntimeException(\u0026#34;Missing crucial field: Order Amount in row: \u0026#34; + row.getRowNumber()); } if (next != null) { next.handle(row, null); } } } // Handler 3 - Save to database\npublic class SaveToDatabaseHandler implements RowHandler { @Override public void handle(OrderRow row, RowHandler next) { databaseService.save(row); // Final handler, no next } } How chain was called:\nRowHandler handlerChain = new ValidateFormatHandler(); handlerChain.handle(row, new ValidateCriticalFieldsHandler() .handle(row, new SaveToDatabaseHandler() ) ); It\u0026rsquo;s now neat. It follows all the prnciples i studied. DRY? Yes dry as hell. SOLID? Of yes super SOLID.\nIt\u0026rsquo;s nice and cool, running without any problems what so ever. But now we our code is about to face a very common issue in software engineering, that is \u0026ldquo;new requiremets\u0026rdquo;.\nFirst requirement, if a non critical field is missing just throw an error and skip processing the entire row and update the status as failed with a reaon, on the status table\nInitially it was a simple requirement validate and save the data. Now i am giong to handle some non critical errors. So the impact on the code?\nEach handler now needs a try catch. Somehow skip the processing of faulty rows Chain was originally designed to throw exceptions — now I had to catch and recover inside the chain. Results.\nI added messy try-catch inside handlers.\nChain became confusing: Should I throw or should I pass silently?\nSmall cracks started appearing.\n@Override public void handle(OrderRow row, RowHandler next) { try { if (row.getCustomerName() == null) { throw new NonCriticalFieldMissingException(\u0026#34;Customer Name is missing in row: \u0026#34; + row.getRowNumber()); } if (row.getDeliveryInstructions() == null) { throw new NonCriticalFieldMissingException(\u0026#34;Delivery Instructions is missing in row: \u0026#34; + row.getRowNumber()); } // Validation passed if (next != null) { next.handle(row, null); } } catch (NonCriticalFieldMissingException e) { System.out.println(\u0026#34;Skipping row \u0026#34; + row.getRowNumber() + \u0026#34;: \u0026#34; + e.getMessage()); // update failure reason, clever and smart move } } } 🧹 What Changed in the Code: Phase 1 Aspect Before Phase 1 After Phase 1 Error Handling Exception would stop processing immediately Exceptions caught, skip faulty row, continue Handler Responsibilities Pure validation and passing to next handler Validation + error handling + deciding whether to continue Code Clarity Very clean and focused Slightly messy, handlers doing multiple things Risk Low (simple logic) Higher — missing catch could cause wrong behavior Debugging Easy to understand flow Hard to trace which row failed where After Phase 1, my neat Chain of Responsibility started leaking.\nEach handler was no longer just responsible for validation — it also had to handle skipping, catching exceptions, and deciding whether the next handler should even be called.\nWhat started as a clean “pass responsibility forward” model became a fragile, try-catch-infested mess.\n☠️ Phase 2: The Beginning of the End Just when I thought my patched-up Chain of Responsibility could survive a few more release cycles, the universe (read: product owner) had other plans.\nThe next requirement dropped like a nuke:\n📜 New Requirement: Stop Everything on Critical Error \u0026ldquo;If a critical field like \u0026lsquo;Order Amount\u0026rsquo; is missing, stop processing the entire file immediately.\u0026rdquo; \u0026ldquo;Update the file status as FAILED with the error message.\u0026rdquo; \u0026ldquo;No skipping. No partial saving. Just die. Immediately.\u0026rdquo; What This Meant for the Code Now each handler needed the power to kill the entire processing flow. No more just skipping rows like earlier. Somehow, one row’s validation failure should abort everything — like pulling the emergency brake on a moving train. 🚂💥 😵 My Reaction At this point, my reaction was:\n\u0026ldquo;I am not going to throw away my precious Chain of Responsibility. No way.\u0026rdquo;\nSo I did what every engineer does when trying to cling onto a broken architecture:\n🛠 I invented a new Exception class. public class CrucialFailureException extends RuntimeException { public CrucialFailureException(String message) { super(message); } } This was my emergency eject button\nSo the handler will look like :\npublic class ValidateCriticalFieldsHandler implements RowHandler { @Override public void handle(OrderRow row, RowHandler next) { if (row.getOrderAmount() == null) { throw new CrucialFailureException(\u0026#34;Critical field \u0026#39;Order Amount\u0026#39; missing in row: \u0026#34; + row.getRowNumber()); } if (next != null) { next.handle(row, null); } } } Now the main loop will be:\nRowHandler handlerChain = new ValidateNonCriticalFieldsHandler();\ntry { for (OrderRow row : uploadedRows) { handlerChain.handle(row, new ValidateCriticalFieldsHandler()); } updateFileStatusAsSuccess(fileId); } catch (CrucialFailureException e) { updateFileStatusAsFailed(fileId, e.getMessage()); System.out.println(\u0026#34;Critical error occurred. Aborting file processing: \u0026#34; + e.getMessage()); } \u0026hellip;\n🧹 What Changed in the Code: Phase 2 Aspect Before Phase 2 After Phase 2 Error Handling One exception type for all validation issues Two exception types: normal vs crucial failure Control Flow Handlers passed forward normally Handlers could throw CrucialFailureException to abort everything Code Clarity Getting slightly messy Now two paths: \u0026ldquo;normal continue\u0026rdquo; and \u0026ldquo;critical death\u0026rdquo; Debugging Traceable with some effort Needed to track who threw which exception at what point Mental State Managing somehow \u0026ldquo;Okay, it\u0026rsquo;s working\u0026hellip; but I\u0026rsquo;m scared to touch it now.\u0026rdquo; 😅 💀 Phase 3: The Final Requirement That Broke Everything 📜 Partial Success Requirement We cannot afford to lose all the good rows just because one bad row exists. If some rows have critical errors: Process whatever rows are valid. Store bad rows into an error table. Allow reprocessing of those rows after corrections. The file overall should still be marked partially successful. Also track how many rows were successful vs failed for reporting. My Final Realization After the \u0026ldquo;Partial Success with Reprocessing\u0026rdquo; requirement hit me like a truck,\nI finally sat back and truly understood:\nI should have just followed simple classes and a simple for-loop.\nThat’s it.\nNo fancy patterns.\nNo hero architecture.\nJust plain, boring, beautiful code.\n🎯 What I Would Do Today I would use plain Java classes — a simple ValidatorService, a DatabaseSaverService, and a StatusUpdater. I would use a straightforward for-loop: Validate each row. Capture errors. Save valid rows. Record partial success/failure cleanly. I would keep each responsibility small and dumb. No complicated object chains. No custom exception magic. No god-like context objects. Just small functions working together quietly like a simple assembly line. 🏭\nReal Wisdom I Learned Elegant design is not what looks good in interviews.\nIt’s what survives requirement changes without becoming a monster. Simple code is not \u0026ldquo;junior code.\u0026rdquo;\nIt’s senior code pretending to be junior so that it can survive. If your design needs a special exception just to keep working, it’s already dying. 🧹 Final Thought \u0026ldquo;When the problem is a simple Excel sheet, you don’t need the Avengers.\nYou just need a broomstick and a dustpan.\u0026rdquo; 🧹\n","permalink":"http://localhost:1313/posts/mindblowing-over-engineering/","summary":"\u003ch1 id=\"the-hidden-pitfalls-of-overusing-design-patterns\"\u003eThe Hidden Pitfalls of Overusing Design Patterns\u003c/h1\u003e\n\u003cp\u003eThis is my story of using the \u003cstrong\u003eChain of Responsibility\u003c/strong\u003e pattern after reading about it in a book while preparing for an interview.\u003cbr\u003e\nYes — I actually used it in a real job. And well, it didn’t turn out the way I had imagined.\u003c/p\u003e\n\u003cp\u003eBefore we dive in, here\u0026rsquo;s a quote from one of my favorite authors, Khaled Hosseini (\u003cem\u003eThe Kite Runner\u003c/em\u003e):\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;There is only one sin, only one. And that is theft. Every other sin is a variation of theft.\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e","title":"Enterprise Over Engineering"},{"content":"എനിക്ക് പലപ്പോഴും തോന്നിയിട്ടുണ്ട് നമ്മുടെ ഏതു ഭാഷയിലാണ് നമ്മളുടെ ബ്രെയിൻ പ്രവർത്തിക്കുന്നത് എന്ന്. ഇന്റർനെറ്റിൽ പരതുമ്പോൾ നമ്മുക്ക് മനസിലാകും ബ്രെയിൻ symbols ഉം മോഡലുകളും ആണ് ഉപയോഗിക്കുന്നത് എന്ന്. പക്ഷെ ഒരു പ്രോബ്ലം സോൾവ് ചെയ്യുമ്പോൾ നമ്മുടെ ബ്രെയിൻ കൂടുതൽ സമയവും ചിലവഴിക്കുന്നത് ഐഡിയകളെ പരിഭാഷ ചെയ്യാൻ ആയിരിക്കും. പ്രത്യേകിച്ച് മലയാളായി ആയ എനിക്ക് ഇതൊരു പ്രശ്നമായി തോന്നിയിട്ടുണ്. ചില പ്രോബ്ലെംസ് സോൾവ് ചെയ്യുമ്പോൾ എനിക്ക് കൂടുതൽ വഴങ്ങുന്ന ഭാഷ അത് മലയാളം ആണ്.\nഉദാഹരണത്തിന് ഈ പ്രോബ്ലം നോക്കൂ. Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining. If you look at Leetcode explanation the language barrier will become very evident in finding a solution to this problem. നമുക്ക് ഒരു height array കൊടുത്തിട്ടുണ്ട്: [0,1,0,2,1,0,1,3,2,1,2,1] ഈ each number ഒരു bar ആണെന്ന് ചിന്തിക്കൂ. മഴ കഴിഞ്ഞശേഷം എത്ര units വെള്ളം ഈ structure trap ചെയ്യാമെന്ന് കാണണം.\n🧠 അടിസ്ഥാന ആശയം: ഒരു position-ൽ വെള്ളം trap ചെയ്യാൻ കഴിയുന്ന അളവ് = min(Left-ൽ കാണുന്ന ഏറ്റവും വലിയ wall, Right-ൽ കാണുന്ന ഏറ്റവും വലിയ wall) - height[i] ഒരു പൊസിഷനിൽ വെള്ളം നിറയണമെങ്കിൽ അതിന്റെ ഇടത്തേയും വലത്തേയും വാളുകളുടെ ഇടയിൽ ആയിരിക്കണം. ഉദാഹരണത്തിന് ഇടതു വശത്തെയോ വലതു വശത്തെയോ വാൾ വലുതായിരിക്കണം.\nഇടതു വശത്തെ വാളിന്റെ ഉയരം രണ്ടും വലതു വശത്തെ വാളിന്റെ ഉയരം മൂന്നും നടുക്ക് നിക്കുന്ന വാളിന്റെ ഉയരം ഒന്നും ആണെന്ന് സങ്കല്പിക്കുക. അപ്പോൾ നടുക്ക് നിക്കുന്ന ഭിത്തിക്ക് ഒരു യൂണിറ്റ് ഹോൾഡ് ചെയ്യാൻ പറ്റും. അതായത് min(Left-ൽ കാണുന്ന ഏറ്റവും വലിയ wall, Right-ൽ കാണുന്ന ഏറ്റവും വലിയ wall) - height[i] നമ്മൾ എവിടെയാണോ നിക്കുന്നത് ആ വാളിന്റെ ഉയരം അതിൽ നിന്നും കുറക്കേണ്ടിയിരിക്കുന്നു. ചെറിയ മതിലാണ് വെള്ളത്തിന്റെ തടഞ്ഞു നിർത്തുന്നത്. അതിനെ ബേസ് ചെയ്താൽ നമ്മുക്ക് വളരെ എളുപ്പത്തിൽ ഇതിനൊരു നല്ല സൊല്യൂഷൻ കണ്ടുപിടിക്കാൻ പറ്റും.\n🎯 ഒറ്റവാക്കിൽ പറഞ്ഞാൽ:\nഒരു വശത്ത് വലിയ മതിൽ ഉണ്ടെങ്കിൽ, നമ്മൾ ചെറിയ വാളിൽ നിന്നും അതിലേക്ക് നീങ്ങികൊണ്ടിരിക്കുക. നീങ്ങുന്ന വഴിക്ക് calculation ചെയ്യുക. ഓർക്കുക ഒരു ചെറിയ മതിലിനു മാത്രമേ വെള്ളം തടയാൻ പറ്റുകയുള്ളു.\nരണ്ടു പോയിന്റർ ഉപയോഗിച് ഇത് ചെയ്യുമ്പോൾ നമ്മുക് ഒരു ഗ്യാരണ്ടി കിട്ടും അതായത് ഒരുവശത്ത് വലിയ മതിൽ ഉറപ്പായിട്ട് കാണുമ്പോൾ, മറ്റേ വശം വെറും limit ആണ്. കോഡിലെ ആദ്യത്തെ ഇഫ് കണ്ടിഷൻ ഈ ഗ്യാരണ്ടീ നമുക്ക് നൽകും if (height[left] \u0026lt; height[right]) {\nമുഴുവൻ കോഡും നിങ്ങൾക് താഴെ കാണാം\n","permalink":"http://localhost:1313/2025/04/blog-post.html","summary":"\u003cp\u003eഎനിക്ക് പലപ്പോഴും തോന്നിയിട്ടുണ്ട് നമ്മുടെ ഏതു ഭാഷയിലാണ് നമ്മളുടെ ബ്രെയിൻ പ്രവർത്തിക്കുന്നത് എന്ന്. ഇന്റർനെറ്റിൽ പരതുമ്പോൾ നമ്മുക്ക് മനസിലാകും ബ്രെയിൻ symbols ഉം മോഡലുകളും ആണ് ഉപയോഗിക്കുന്നത് എന്ന്. \u003c/p\u003e\n\u003cp\u003eപക്ഷെ ഒരു പ്രോബ്ലം സോൾവ് ചെയ്യുമ്പോൾ നമ്മുടെ ബ്രെയിൻ കൂടുതൽ സമയവും ചിലവഴിക്കുന്നത് ഐഡിയകളെ പരിഭാഷ ചെയ്യാൻ ആയിരിക്കും. പ്രത്യേകിച്ച് മലയാളായി ആയ എനിക്ക് ഇതൊരു പ്രശ്നമായി തോന്നിയിട്ടുണ്. ചില പ്രോബ്ലെംസ് സോൾവ് ചെയ്യുമ്പോൾ എനിക്ക് കൂടുതൽ വഴങ്ങുന്ന ഭാഷ അത് മലയാളം ആണ്.\u003c/p\u003e\n\u003cp\u003e ഉദാഹരണത്തിന് ഈ പ്രോബ്ലം നോക്കൂ. Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining. If you look at Leetcode explanation the language barrier will become very evident in finding a solution to this problem. നമുക്ക് ഒരു height array കൊടുത്തിട്ടുണ്ട്: [0,1,0,2,1,0,1,3,2,1,2,1] ഈ each number ഒരു bar ആണെന്ന് ചിന്തിക്കൂ. മഴ കഴിഞ്ഞശേഷം എത്ര units വെള്ളം ഈ structure trap ചെയ്യാമെന്ന് കാണണം.\u003c/p\u003e","title":"മധുരം മലയാളം. ഒരു മലയാളം പ്രോബ്ലം സോൾവിങ് "},{"content":"Managing Concurrent Updates with Distributed Locks\nManaging Concurrent Updates with Distributed Locks In distributed systems, managing concurrent access to shared resources is crucial to ensure data consistency and prevent corruption. Let’s explore how to handle this using a Java example, starting with a basic implementation and improving it step-by-step.\nBasic Implementation Without Proper Lock Handling Here\u0026rsquo;s a simple version of a method that tries to acquire a distributed lock, perform an update, and release the lock:\npublic void updateResource() { try { // Attempt to acquire the distributed lock acquireLock(); // Perform the update operation doUpdate(); } catch (LockAcquisitionException e) { // Handle the case where the lock could not be acquired System.out.println(\u0026#34;Failed to acquire lock: \u0026#34; + e.getMessage()); } catch (Exception e) { // Handle any other exceptions that occurred during the update operation e.printStackTrace(); } finally { // Ensure the lock is released in the finally block unlock(); } } private void acquireLock() throws LockAcquisitionException { // Implement your logic to acquire a distributed lock here // Throw LockAcquisitionException if the lock cannot be acquired } private void unlock() { // Implement your logic to release the distributed lock here } private void doUpdate() { // Implement your update logic here } // Custom exception for lock acquisition failure public class LockAcquisitionException extends Exception { public LockAcquisitionException(String message) { super(message); } } What Happens with Concurrent Threads? Imagine three threads (A, B, and C) are running this method at the same time:\nThread A acquires the lock and starts updating the resource. Thread B tries to acquire the lock but fails because Thread A is holding it, so it throws an exception. Thread B incorrectly releases the lock in the finally block, it will release the lock that Thread A is still using. Thread C then acquires the lock, potentially corrupting the data if it starts updating the resource while Thread A is still working. Can We Use Synchronized Methods? In a single JVM (Java Virtual Machine), you can use the synchronized keyword to ensure only one thread can execute the method at a time:\npublic synchronized void updateResource() { try { // Perform the update operation doUpdate(); } catch (Exception e) { // Handle exceptions e.printStackTrace(); } } However, this doesn\u0026rsquo;t work in a distributed system where multiple JVM instances might be running on different servers. The synchronized keyword only works within the same JVM.\nThe Lost Update Problem The \u0026ldquo;lost update\u0026rdquo; problem occurs when multiple threads read the same data, modify it, and write it back, overwriting each other\u0026rsquo;s changes. For example, if Thread A reads a value and then Thread B reads the same value, both might update the value and save it, resulting in one of the updates being lost.\nProper Lock Handling To handle this correctly in a distributed system, we need to ensure that the lock is only released if it was successfully acquired. Here’s how to do it:\npublic void updateResource() { boolean lockAcquired = false; // Flag to track if the lock was acquired try { // Attempt to acquire the distributed lock acquireLock(); lockAcquired = true; // Set the flag to true if the lock is acquired // Perform the update operation doUpdate(); } catch (LockAcquisitionException e) { // Handle the case where the lock could not be acquired System.out.println(\u0026#34;Failed to acquire lock: \u0026#34; + e.getMessage()); } catch (Exception e) { // Handle any other exceptions that occurred during the update operation e.printStackTrace(); } finally { // Ensure the lock is released in the finally block if it was acquired if (lockAcquired) { unlock(); } } } private void acquireLock() throws LockAcquisitionException { // Implement your logic to acquire a distributed lock here // Throw LockAcquisitionException if the lock cannot be acquired } private void unlock() { // Implement your logic to release the distributed lock here } private void doUpdate() { // Implement your update logic here } // Custom exception for lock acquisition failure public class LockAcquisitionException extends Exception { public LockAcquisitionException(String message) { super(message); } } Explanation of the Solution Flag Initialization: A boolean flag lockAcquired is set to false initially. This flag will be set to true only if the lock is successfully acquired by the thread. Lock Acquisition: The acquireLock method attempts to acquire the lock. If it fails, it throws a LockAcquisitionException. Flag Update: If the lock is successfully acquired, lockAcquired is set to true. Update Operation: The doUpdate method performs the necessary update operations while holding the lock. Exception Handling: LockAcquisitionException: Handles the scenario where the lock acquisition fails. General Exception Handling: Catches any other exceptions that might occur during the update operation. Finally Block: The finally block checks if the lockAcquired flag is true before calling the unlock method. This ensures that the lock is only released if it was successfully acquired, preventing any attempt to release a lock that wasn’t acquired. Conclusion By managing the acquisition and release of distributed locks properly, you can ensure that your application handles concurrent updates safely and efficiently. This approach prevents race conditions and lost updates, maintaining data consistency across distributed systems. Using a boolean flag to track lock acquisition ensures that the lock is only released if it was acquired, preventing potential data corruption and ensuring robust lock management.\n","permalink":"http://localhost:1313/2024/05/managing-concurrent-updates-with.html","summary":"\u003cp\u003eManaging Concurrent Updates with Distributed Locks\u003c/p\u003e\n\u003ch1 id=\"managing-concurrent-updates-with-distributed-locks\"\u003eManaging Concurrent Updates with Distributed Locks\u003c/h1\u003e\n\u003cp\u003eIn distributed systems, managing concurrent access to shared resources is crucial to ensure data consistency and prevent corruption. Let’s explore how to handle this using a Java example, starting with a basic implementation and improving it step-by-step.\u003c/p\u003e\n\u003ch2 id=\"basic-implementation-without-proper-lock-handling\"\u003eBasic Implementation Without Proper Lock Handling\u003c/h2\u003e\n\u003cp\u003eHere\u0026rsquo;s a simple version of a method that tries to acquire a distributed lock, perform an update, and release the lock:\u003c/p\u003e","title":"Managing Concurrent Updates with Distributed Locks"},{"content":" The main problem with me w.r.t DP problem is forgetting it if i don\u0026#39;t practice. So I decided to revisit all the DP problems i solved once again just to refresh my memory. So first Lets start with a simple one. Here we go!!! Climbing stairs from Leetcode. Explanation : You are climbing a staircase. It takes n steps to reach the top. Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top? Example 1: Input: n = 2 Output: 2 Explanation: There are two ways to climb to the top. 1\\. 1 step + 1 step 2\\. 2 steps Example 2: Input: n = 3 Output: 3 Explanation: There are three ways to climb to the top. 1\\. 1 step + 1 step + 1 step 2\\. 1 step + 2 steps 3\\. 2 steps + 1 step So how can we solve this simple problem? As usual we can solve this problem using recursion. But the problem with recursion is it will have exponential time complexity. In recursive way the following is the solution \\`\\`\\`java public int climbStairs(int n) { if(n == 1) return 1; if(n == 2) return 2; return climbStairs(n-1) + climbStairs(n-2); } \\`\\`\\` The logic of the above is : \\* If n is 1 then there is only one way to climb the stair. \\* If n is 2 then there are two ways to climb the stair. \\* If n is greater than 2 then the number of ways to climb the stair is the sum of the number of ways to climb the stair when n-1 and n-2. So we recursively call climbStairs(n-1) and climbStairs(n-2) and\nadd them to get the result. What is the problem with this approach? The problem with this approach is it has exponential time complexity. For example if n = 5 then the number of ways to climb the stair is 8. So the number of recursive calls will be 8. If n = 6 then the number of ways to climb the stair is 13. So the number of recursive calls will be 13. if n=7 then the number of ways to climb the stair is 21. So the number of recursive calls will be 21. So the time complexity of this approach is O(2^n). Where n is the number of stairs. and 2 is the number of ways to\nclimb the stair. If we construct a recursion tree for n=5 it will look like below. !\\[\\](img\\_1.png) Space complexity of this approach is O(n) where n is the number of stairs. Or in other words the depth of the recursion tree is n. So lets get into the basic dynamic programming approach. \\`\\`\\`java class Solution { // A function that represents the answer to the problem for a given state private int dp(int i) { if (i \u0026lt;= 2) return i; // Base cases return dp(i - 1) + dp(i - 2); // Recurrence relation } public int climbStairs(int n) { return dp(n); }} \\`\\`\\` The above approach is a top-down approach. Means we are solving the problem from the top to the bottom. The time complexity of the above approach is\nO(2^n) and the space complexity is O(n). **_So this is not really dynamic programming._** This is just a recursive approach. So lets get into the bottom-up approach. ```java\nclass Solution {\npublic int climbStairs(int n) { if (n \u0026lt;= 2) return n; // Base cases int[] dp = new int[n + 1]; // Create an array to store\n// the subproblems dp\\[1\\] = 1; // Base case dp\\[2\\] = 2; // Base case for (int i = 3; i \u0026lt;= n; i++) { dp\\[i\\] = dp\\[i - 1\\] + dp\\[i - 2\\]; // The recurrence relation } return dp\\[n\\]; // The answer to the problem for n steps }} \\`\\`\\` The above approach is a bottom-up approach. Means we are solving the problem from the bottom to the top. Also the above approach is a tabulation approach.\nMeans we are solving the problem using an array to store the subproblems. The time complexity of the above approach is O(n) and the space\ncomplexity is O(n). The flow goes like this : * dp[1] = 1 // Base case Means if there is only one stair then\nthere is only one way to climb the stair.\n* dp[2] = 2 // Base case Means if there are two stairs then\nthere are two ways to climb the stair.\nfrom 3 to n we calculate the number of ways to climb the stair\nusing the following formula.\n\\* dp\\[i\\] \\= dp\\[i - 1\\] \\+ dp\\[i - 2\\] // The recurrence relation \\* dp\\[n\\] // The answer to the problem for n steps So if you look at the first example of recursive approach the number of recursive calls for n=5 is 8. But if you look at the bottom-up\napproach the number of steps to calculate the number of ways to climb the stair for n=5 is 5. So the time complexity of the bottom-up approach is O(n)\nand the space complexity is O(n). So lets get into another problem. This is a Medium one.\nAnd this is a classic one.\nThe problem is House Robber from Leetcode.\nYou are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that\nadjacent houses have security systems connected and it will\nautomatically contact the police if two adjacent houses were broken into on\nthe same night.\nBase case : If there is 4 houses then the maximum amount of money you can rob is the maximum of the amount of money in the first house and the amount of money in the second house. plus the maximum amount of money you can rob from the remaining houses. The intuition behind the problem is the following.\nmaxRobbedAmount[0]=nums[0]\nmaxRobbedAmount[1]=max(maxRobbedAmount[0],nums[1])\nmaxRobbedAmount[2]=max(maxRobbedAmount[0]+nums[2],maxRobbedAmount[1])\nmaxRobbedAmount[3]=max(maxRobbedAmount[1]+nums[3],maxRobbedAmount[2])\nmaxRobbedAmount[4]=max(maxRobbedAmount[2]+nums[4],maxRobbedAmount[3])\nEquation : maxRobbedAmount[i]=max(maxRobbedAmount[i-2]+nums[i],\nmaxRobbedAmount\\[i-1\\]) So the solution is the following. \\`\\`\\`java class Solution { public int rob(int\\[\\] nums) { if (nums.length == 0) return 0; if (nums.length == 1) return nums\\[0\\]; if (nums.length == 2) return Math.max(nums\\[0\\], nums\\[1\\]); int\\[\\] maxRobbedAmount = new int\\[nums.length\\]; maxRobbedAmount\\[0\\] = nums\\[0\\]; maxRobbedAmount\\[1\\] = Math.max(nums\\[0\\], nums\\[1\\]); for (int i = 2; i \u0026lt; nums.length; i++) { maxRobbedAmount\\[i\\] = Math.max(maxRobbedAmount\\[i - 2\\] + nums\\[i\\], maxRobbedAmount\\[i - 1\\]); } return maxRobbedAmount\\[nums.length - 1\\]; }} ```\nThe above approach is a bottom-up approach. Means we are solving the\nproblem from the bottom to the top. Also the above approach is a\ntabulation approach. Means we are solving the problem using an array to\nstore the subproblems.So the time complexity of the above approach is O(n)\nand the space complexity is O(n). Because we are using an array to store\nthe subproblems.\n``` ","permalink":"http://localhost:1313/2024/03/httpswww.justinmathew.com202403dynamic-programming-revisiting.html.html","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\n\nThe main problem with me w.r.t DP problem is forgetting it if i don\u0026#39;t \n\npractice. So I decided to revisit all the DP problems i solved once again \n\njust to refresh my memory.\n\n  \nSo first Lets start with a simple one. Here we go!!!  \n  \nClimbing stairs from Leetcode.  \n  \n\nExplanation : You are climbing a staircase. \n\nIt takes n steps to reach the top.  \nEach time you can either climb 1 or 2 steps. In how many \n\ndistinct ways can you climb to the top?\n\nExample 1:  \n  \nInput: n = 2  \nOutput: 2  \nExplanation: There are two ways to climb to the top.  \n1\\. 1 step + 1 step  \n2\\. 2 steps  \n  \nExample 2:  \nInput: n = 3  \nOutput: 3  \nExplanation: There are three ways to climb to the top.  \n  \n1\\. 1 step + 1 step + 1 step  \n2\\. 1 step + 2 steps  \n3\\. 2 steps + 1 step  \n  \nSo how can we solve this simple problem?  \n  \nAs usual we can solve this problem using recursion.   \nBut the problem with recursion is it will have exponential time complexity.   \nIn recursive way the following is the solution     \\`\\`\\`java  \n public int climbStairs(int n) { if(n == 1) return 1; if(n == 2) return 2; return climbStairs(n-1) + climbStairs(n-2); } \\`\\`\\`  \nThe logic of the above is : \n\n \\* If n is 1 then there is only one way to climb the stair. \\* If n is 2 then there are two ways to climb the stair. \\* If n is greater than 2 then the number of ways to climb the stair is  the sum of the number of ways to climb the stair when n-1 and n-2.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSo we recursively call climbStairs(n-1) and climbStairs(n-2) and\u003c/p\u003e","title":"Revisiting Dynamic Programming"},{"content":"Design patterns are general reusable solutions to common problems that occur in software design. They are not code, but rather guidelines on how to solve a particular problem in a particular context. They are not a finished design that can be transformed directly into code. They are a description or template for how to solve a problem that can be used in many different situations. Types of design patterns in Java -------------------------------- There are three types of design patterns in Java: * Creational\n* Structural\n* Behavioral\n### Creational design patterns Creational design patterns are related to the way of creating objects. These patterns provide various object creation mechanisms, which increase flexibility and reuse of existing code. These patterns provide various object creation mechanisms, which increase flexibility and reuse of existing code. There are five types of creational design patterns: \\* Singleton Method \\* Factory Method \\* Abstract Factory Method \\* Builder Method \\* Prototype Method ### Singleton Method Consider a scenario where you need to manage a configuration file. You need to read the configuration file only once and then keep it in memory. For this, you need to create a class that will read the configuration file and keep it in memory. The below is a PUML diagram for a simple Configuration manager class. Example: Java Runtime, Logger, Spring Beans, and many more.\nDifferent flavours from JDK.\nRuntime.getRuntime();\nDesktop.getDesktop();\n","permalink":"http://localhost:1313/2024/03/java-design-patterns-jdp.html","summary":"\u003ch2 id=\"design-patterns-are-general-reusable-solutions-to-common-problems-that-occur-in-software-design-they-are-not-code-but-rather-guidelines-on-how-to-solve-a-particular-problem-in-a-particular-context-they-are-not-a-finished-design-that-can-be-transformed-directly-into-code-they-are-a-description-or-template-for-how-to-solve-a-problem-that-can-be-used-in-many-different-situations\"\u003eDesign patterns are general reusable solutions to common problems that occur in software design. They are not code, but rather guidelines on how to solve a particular problem in a particular context. They are not a finished design that can be transformed directly into code. They are a description or template for how to solve a problem that can be used in many different situations.\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e  \n  \n\nTypes of design patterns in Java\n--------------------------------\n\n  \nThere are three types of design patterns in Java:  \n  \n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e* Creational\u003cbr\u003e\n* Structural\u003cbr\u003e\n* Behavioral\u003c/p\u003e","title":"Java Design Patterns - JDP Series #1"},{"content":"Today was not an ordinary day for me; it began with the need to take an unexpected sick day. The morning unfolded with urgency as I planned to drop off my child, but fate had a different story in store. Around 9:30 AM, near Hopefarm in Whitefield, my day took a dramatic turn. As I attempted to overtake a car on the right, an elderly man crossing the road appeared in my path. Despite his slow pace and my high speed, I managed to brake just in time, reducing the impact of the collision. The man, although not severely injured, fell and sustained minor scratches on his knees.\nThe incident drew immediate attention, and several passersby stopped to assist. My initial fear of confrontation was alleviated by my attempts to communicate in broken Kannada, which, to my relief, helped manage the situation. With my hands trembling, I turned to my wife and daughter, who were with me in the car, for a water bottle to tend to the injured man. After cleaning his face, we hurried him to Manipal Hospital for medical attention.\nRemarkably, the man expressed his gratitude throughout the ordeal. Upon arrival at the hospital, initial examinations showed his heart rate was normal, but what shocked me was learning about his age, 75, and that he had two pacemakers. His resilience was notable, yet the concern for potential injuries due to his age was palpable. Fortunately, after further assessments and an X-ray, we were relieved to hear that there were no fractures, only a knee injury.\nThroughout this time, the man remained conscious and trusting. His ability to call his son-in-law and wife brought a sense of comfort to him, and I did my best to provide reassurance. As we awaited the X-ray results, my brother joined me, offering his support. The news that there were no fractures brought a collective sigh of relief.\nThe atmosphere grew tense with the arrival of his wife and son-in-law, bracing themselves for the worst, fearing the accident had left him in a dire state. However, their anxiety was soon alleviated when they realized that, thanks to the prompt and compassionate actions taken, he was in stable condition. The moment served as a poignant reminder of the fragile nature of life and the importance of kindness in times of crisis.\nAs conversations unfolded, the wife began to share more about their family\u0026rsquo;s journey, particularly focusing on their son\u0026rsquo;s challenges. Their son, once a manager at IBM, had taken a bold leap by moving to Singapore for work before returning to Bangalore. His career transition from the corporate world to starting his own real estate business marked a period of prosperity for the family. However, life\u0026rsquo;s unpredictable nature struck hard when he suffered a stroke, an event that turned their lives upside down.\nAt just 45 years old, their son faced a daunting battle for recovery. The medical expenses were astronomical, draining nearly 45 to 50 lakhs from their savings, with hospital bills averaging around 90,000 per day over two months. Despite the financial strain, the progress he made was heartening; from being bedridden to sitting up, each small victory was celebrated. Yet, the emotional and financial toll of the situation was evident as his mother recounted their story, her eyes brimming with tears.\nThe narrative of their son\u0026rsquo;s struggle and resilience added a layer of depth to the day\u0026rsquo;s events, transforming a simple act of assistance into a profound connection between two families. The gratitude expressed by the elderly man\u0026rsquo;s wife, punctuated by her blessing, \u0026ldquo;God Bless you and your family,\u0026rdquo; resonated deeply, leaving an indelible mark on the heart.\nIn the end, as I ensured their safe return home and bid them farewell, the promise to keep their loved one safe from harm was not just a commitment to them but a reminder of the interconnectedness of our lives. The story of their son, a testament to human resilience in the face of adversity, underscored the day\u0026rsquo;s experiences, enriching the fabric of this unexpected encounter with strands of hope, perseverance, and shared humanity.\n","permalink":"http://localhost:1313/2024/02/fed-27-2024-day-of-compassion-and.html","summary":"\u003cp\u003eToday was not an ordinary day for me; it began with the need to take an unexpected sick day. The morning unfolded with urgency as I planned to drop off my child, but fate had a different story in store. Around 9:30 AM, near Hopefarm in Whitefield, my day took a dramatic turn. As I attempted to overtake a car on the right, an elderly man crossing the road appeared in my path. Despite his slow pace and my high speed, I managed to brake just in time, reducing the impact of the collision. The man, although not severely injured, fell and sustained minor scratches on his knees.\u003c/p\u003e","title":"Fed 27 2024. A Day of Compassion and Unexpected Encounters"},{"content":"Jump Game Peak and Valley approach. Jump game is a medium level leetcode problem which is very interesting yet brainy. Once you understand the problem correctly then the answer is obvious. The probelm goes like this according to leetcode. You are given an integer array nums. You are initially positioned at the array\u0026rsquo;sfirst index, and each element in the array represents your maximum jump length at that position.\nReturn true_ if you can reach the last index, or false otherwise_.\nFor example consider this array\n[1,3,2,2,0,1]\nWhen I am at position 1 then i can jump to position 2, Once i am at postition 2 I can jump to 3,4 or 5. The max i can jump from Second position is to position 5 and the element at position 5 is 0 this essentialy means i cannot jump further. The way to solve this problem using peak and valley here I am representing this array as peaks and valleys.\nIf you look at the above image you can see peak and valleys. Once you are on the valley that is 0 You don\u0026rsquo;t really have a way out of it. But you can actually make a jump from position 3 to position 5 by skipping the valley. This is the basic intuition behind the solution. The basic steps are the following :\nYou create a variable called reachable which holds the maximum position you can reach. So how do you calculate the value of reachable it is :\nMax(reachable, currentPosition+input[currentPosition]).\nSo the max reachable index will be the maximum of reachable or currentPosition+input[currentPosition]. So our job is just to calculate this every position and if at any given point we realize that the currentPosition is greater than the reachable then we reuturn false, essentially means we are at the valley and no way to escape.\nprivate boolean canJumpFromPosition(int i, int\\[\\] nums) { int reachable=0; int n=nums.length; for(int j=0;j\u0026lt;n;j++){ if(j\u0026gt;reachable){ return false; }else{ reachable=Math.max(reachable,j+nums\\[j\\]); } } return true; } In this we iterate over the arrray and each position we check are we at a reachable position or not. If we are not at a reachable position then we are at valley and if we are at a reachable position then we just recalculate the reachable index by adding currentIndex and the value in the current index.\n","permalink":"http://localhost:1313/2024/02/jump-game.html","summary":"\u003ch2 id=\"jump-game-peak-and-valley-approach\"\u003eJump Game Peak and Valley approach.\u003c/h2\u003e\n\u003cp\u003eJump game is a medium level leetcode problem which is very interesting yet brainy. Once you understand the problem correctly then the answer is obvious. The probelm goes like this according to leetcode. \u003c/p\u003e\n\u003cp\u003eYou are given an integer array \u003ccode\u003enums\u003c/code\u003e. You are initially positioned at the array\u0026rsquo;sfirst index, and each element in the array represents your maximum jump length at that position.\u003c/p\u003e\n\u003cp\u003eReturn \u003ccode\u003etrue\u003c/code\u003e_ if you can reach the last index, or \u003cem\u003e\u003ccode\u003efalse\u003c/code\u003e\u003c/em\u003e otherwise_.\u003c/p\u003e","title":"Jump Game"},{"content":"Skewed workloads and Relieving Hot Spots Imagine you have a library with books categorized by their first letter (A-Z). This is like partitioning data based on a key (like the first letter of a book title).\nProblem: One letter (say, \u0026ldquo;X\u0026rdquo;) becomes super popular (a celebrity author!). Everyone wants to read \u0026ldquo;X\u0026rdquo; books, causing a \u0026ldquo;hot spot\u0026rdquo; (overcrowding) in the \u0026ldquo;X\u0026rdquo; section.\nHashing doesn\u0026rsquo;t fix it: Even if you assign different \u0026ldquo;buckets\u0026rdquo; based on a hash of the title, all \u0026ldquo;X\u0026rdquo; books still end up in the \u0026ldquo;X\u0026rdquo; bucket.\nSolution 1: Split the key: Add a random number to each \u0026ldquo;X\u0026rdquo; book title (e.g., \u0026ldquo;X123\u0026rdquo;, \u0026ldquo;X456\u0026rdquo;). This spreads them across different buckets, reducing the crowd in \u0026ldquo;X\u0026rdquo;.\nDrawback: Now you need to check all buckets with \u0026ldquo;X\u0026rdquo; to find all the books (more work for reading).\nSolution 2 (future): Imagine the library magically adjusts shelves based on popularity, automatically spreading out the \u0026ldquo;X\u0026rdquo; books.\nTakeaway: Choose the solution that best fits your needs. Splitting keys helps with hot spots but adds complexity. Future systems might handle this automatically.\nPartitioning and Secondary Indexes A secondary index usually doesn\u0026rsquo;t identify a record uniquely but rather is a way of searching for occurences of a particular value; find all action by user 123, find all articles containing the word hogwash, find all cars whose color is red. In these statements you can see we use a secondary index for our query. This is essential for any database design. The issue with secondary indexes are they don\u0026rsquo;t map neatly to partitions. This happens due to non uniqueness, for example find all action by user means find all actions| filter by user, so the find all will go search on all partition. This is a simple example for this problem.\nThere are two main approaches to partitioning a database with secondary indexes : document-based partitioning and term based partitioning. Partitioning Secondary Indexes by Document. If you look at this image you can see within each partition there is another secondary index created. So if you want to search for red cars, you need to send the query to all partitions, and combine all the results you get back. In this approach you can notice each partition creates its own secondary index and when you are writing to it you only need to deal with the partition that contains the document ID that you are writing. For this exact reason a Document-Partitioned index is also known as local index.\nQuerying this kind of partitioned database is known as scatter/gather, and it can make read queries on secondary indexe quite expensive. Even if you query the partition in parallel _scatter/gather _ is prone to tail latency amplifications. Nevertheless it is a widely used approach. On the next blog we will learn more about partitioining from Martin Klepmann\u0026rsquo;s \u0026ldquo;Designing Data intensive Applications\n","permalink":"http://localhost:1313/2024/02/from-painful-tables-to-performance_15.html","summary":"\u003ch2 id=\"skewed-workloads-and-relieving-hot-spots\"\u003eSkewed workloads and Relieving Hot Spots\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoJClAUTSTj0roZIBCR4_qM3sXJihaJLvPpviq5z25FrwP_N-Tv_SipKyDUAISzWKIb2gRY52GxbIx6uQ71HQaTjjW9UEhJuNiYc1X3fBfcCWRvjE7zTz-3l6YXKBxLFNDtTCb4GXpk2BxGLPc9IJAm_dRsWJpE4JWF2UiTR1WJF9TQFwW7wng8IikazsH/s1024/DALL%C2%B7E%202024-02-16%2010.03.06%20-%20Create%20an%20abstract,%20visually%20engaging%20image%20that%20symbolizes%20the%20concept%20of%20database%20partitioning.%20Use%20geometric%20shapes,%20lines,%20and%20vibrant%20colors%20to%20d.webp\"\u003e\u003cimg loading=\"lazy\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoJClAUTSTj0roZIBCR4_qM3sXJihaJLvPpviq5z25FrwP_N-Tv_SipKyDUAISzWKIb2gRY52GxbIx6uQ71HQaTjjW9UEhJuNiYc1X3fBfcCWRvjE7zTz-3l6YXKBxLFNDtTCb4GXpk2BxGLPc9IJAm_dRsWJpE4JWF2UiTR1WJF9TQFwW7wng8IikazsH/s320/DALL%C2%B7E%202024-02-16%2010.03.06%20-%20Create%20an%20abstract,%20visually%20engaging%20image%20that%20symbolizes%20the%20concept%20of%20database%20partitioning.%20Use%20geometric%20shapes,%20lines,%20and%20vibrant%20colors%20to%20d.webp\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eImagine you have a library with books categorized by their first letter (A-Z). This is like partitioning data based on a key (like the first letter of a book title).\u003c/p\u003e\n\u003cp\u003eProblem: One letter (say, \u0026ldquo;X\u0026rdquo;) becomes super popular (a celebrity author!). Everyone wants to read \u0026ldquo;X\u0026rdquo; books, causing a \u0026ldquo;hot spot\u0026rdquo; (overcrowding) in the \u0026ldquo;X\u0026rdquo; section.\u003c/p\u003e\n\u003cp\u003eHashing doesn\u0026rsquo;t fix it: Even if you assign different \u0026ldquo;buckets\u0026rdquo; based on a hash of the title, all \u0026ldquo;X\u0026rdquo; books still end up in the \u0026ldquo;X\u0026rdquo; bucket.\u003c/p\u003e","title":"From Painful Tables to Performance Bliss: My Journey with Database Partitioning - Part II"},{"content":"Ah, the early days of wrangling massive data tables! I vividly remember the struggle – slow queries, performance bottlenecks, and the ever-growing cloud bill. It was an uphill battle until we unearthed the magic bullet: database partitioning. Talk about a revelation! This newfound approach not only eradicated performance issues but also slashed computational costs.\nBut the story doesn\u0026rsquo;t end there. My exploration revealed a treasure trove of partitioning techniques, each unlocking unique advantages. Inspired by \u0026ldquo;Designing Data-Intensive Applications\u0026rdquo;, I embarked on a quest to master this data management superpower. This blog chronicles my learnings, shedding light on the various ways you can partition your data for optimal performance and cost-efficiency.\nEvery partition is a small database of its own. Each piece of data will belong to one partition. The main reason for having this small databases or we call partition is for scalability. Different partition can be placed in different nodes in a shared-nothing cluster(A cluster of nodes where nodes will be independent).\nAssume there is a query to fetch a row, the query will be performed by a node on its on partition. So to increase throughput just add more nodes.\nPartitioning and Replication Building on the previous statement, if every node has a partition, it implies that each node holds copies of all partitions. This ensures that even though each record belongs to a specific partition, it might still be present on multiple nodes for the sake of fault tolerance. Partitioning by Key Range Real-World Example: Imagine millions of IoT devices, each with a unique IMEI number and timestamps for sensor readings. We can leverage key-range partitioning by using the date as the key. However, a potential drawback arises: all data within a single day would reside in the same partition, leaving others idle. Dual Partitioning: To address this and distribute the write load more evenly, we can introduce dual partitioning. We\u0026rsquo;ll use both the date and the IMEI number as keys. This ensures data gets distributed across multiple partitions based on both date and device, preventing overloading of single partitions.\nQuerying: When fetching data from multiple sensors within a specific time range, separate range queries will be needed for each IMEI number. However, the overall performance gain often outweighs this drawback due to the efficient retrieval within each partition.\nPersonal Experience: This concept played a pivotal role in my career, saving the company millions. By implementing dual partitioning for range-based queries on massive IoT sensor data, we significantly improved performance and optimized resource utilization. This experience solidified the importance of understanding core concepts before designing any system.\nPartitioning by Hash of Key This is another very interesting partitioning concept. In contrast to key-range partitioning, we use a hash function to determine the partition of a given key. A good hash function takes skewed data and makes it uniformly distributed. Imagine you have a 32-bit hash function that takes a string. When you provide a new string, it returns a seemingly random number between 0 and (2 to the power of 32) - 1. Even if the input strings are very similar, their hashes are evenly distributed across that range of numbers.\nCassandra and Mongo uses MD5\nVoldermort uses the Fowoler-Noll-Vo function.\nBuilt-in hash functions of programming languages may not be suitable for data partitioning. For example, Java\u0026rsquo;s hashCode() method can generate different hash values for the same key in different processes. In such cases, it\u0026rsquo;s recommended to use separate hash implementations specifically designed for consistent key distribution across partitions. The figure below shows how partitioning by a well-chosen hash function actually works.\nThis is indeed a great technique for evenly distributing keys across partitions. The partition boundaries can be equally spaced or chosen pseudorandomly.\n","permalink":"http://localhost:1313/2024/02/from-painful-tables-to-performance.html","summary":"\u003cp\u003eAh, the early days of wrangling massive data tables! I vividly remember the struggle – slow queries, performance bottlenecks, and the ever-growing cloud bill. It was an uphill battle until we unearthed the magic bullet:  database partitioning. Talk about a revelation! This newfound approach not only eradicated performance issues but also slashed computational costs.\u003c/p\u003e\n\u003cp\u003eBut the story doesn\u0026rsquo;t end there. My exploration revealed a treasure trove of partitioning techniques, each unlocking unique advantages. Inspired by \u0026ldquo;Designing Data-Intensive Applications\u0026rdquo;, I embarked on a quest to master this data management superpower. This blog chronicles my learnings, shedding light on the various ways you can partition your data for optimal performance and cost-efficiency.\u003c/p\u003e","title":"From Painful Tables to Performance Bliss: My Journey with Database Partitioning - Part I"},{"content":" Introduction A colleague recommended Martin Kleppmann\u0026rsquo;s \u0026ldquo;Designing Data-Intensive Applications\u0026rdquo; to me. Initially, I found the beginning somewhat tedious and opted for a non-linear approach, selecting topics of interest at random rather than reading from start to finish as one might with a novel. This strategy seemed fitting given the book\u0026rsquo;s comprehensive coverage of software system design, akin to an engineering bible. Today, I\u0026rsquo;ve chosen to delve into the concept of Event Sourcing. Let\u0026rsquo;s explore this topic together.\nWhy Event Sourcing?\nRelevance: It\u0026rsquo;s a critical concept in building scalable, resilient distributed systems and is widely applicable in modern software architectures, including microservices. Foundational Knowledge: Understanding event sourcing will deepen your knowledge of how large-scale systems manage state and handle data changes over time. Practical Application: It\u0026rsquo;s highly relevant to real-world systems, particularly in scenarios requiring audit trails, historical data analysis, or complex business transactions. Lets see an Order table schema below which is written on the traditional approach.\nTraditional Approach schema\nHere :\norder_id is a unique identifier for each order. customer_id links to a customers table (not shown) that contains customer details. order_date is the date and time the order was placed. total_amount is the total cost of the order. status could be values like \u0026lsquo;Placed\u0026rsquo;, \u0026lsquo;Paid\u0026rsquo;, \u0026lsquo;Shipped\u0026rsquo;, \u0026lsquo;Delivered\u0026rsquo;, etc. Simple right..?\nObviously you have seen this kind of schema in your career. 100% , All of us seen this.\nEvent Sourcing Approach Schema\nAnd a simplified orders table to store order IDs\nYou are starting to get what is event sourcing is now. But still doubtfull right, well everyone is will be doubtful until you see some real data. So lets insert some dummy data into the traditional table and to the event sourcing table. This will create four orders with different statuses in the orders table.\nNow the event sourcing data.\nIn this example Order 1 goes through the full lifecycle from being placed to delivery. Order 2 is placed and paid for but hasn\u0026rsquo;t been shipped yet. Order 3 is placed, paid for, and shipped, but not yet delivered. Order 4 is only placed. Event sourcing is much more descriptive when it comes to the historical events that have happened to an order. Imagine a bug occurred during the order placing flow; we can always replay this particular event and debug the system, instead of permanently losing that flow and saying \u0026ldquo;not reproducible,\u0026rdquo; then waiting for the next order placement to occur. You know, the infamous \u0026ldquo;not reproducible\u0026rdquo; bug.\nThe ideal places to apply this?\nConsidering the earlier discussion about the \u0026ldquo;not reproducible\u0026rdquo; bug, let\u0026rsquo;s contemplate its implications in more critical scenarios. Would a bank ever be content with labeling a transactional glitch as \u0026ldquo;not reproducible\u0026rdquo; and leaving it at that? Or imagine placing an order on Amazon, only to encounter an error during packaging that erroneously signals a payment problem. Would such explanations be satisfactory?\nEvent sourcing is well-suited for systems that undergo a series of discrete events, each affecting the system\u0026rsquo;s state.\nThis is just an introducton : Please watch/read the following videos to understand more\nhttps://www.youtube.com/watch?v=MA_3fPBFBtg. About linkedins uses of kafka\nEvent sourcing from martin kleppmann designing data-intensive applications\nSo that\u0026rsquo;s 10 Minutes in the morning!\n","permalink":"http://localhost:1313/2024/02/event-sourcing-moving-out-of-traditions.html","summary":"\u003ch2 id=\"introduction\"\u003e Introduction\u003c/h2\u003e\n\u003cp\u003eA colleague recommended Martin Kleppmann\u0026rsquo;s \u0026ldquo;Designing Data-Intensive Applications\u0026rdquo; to me. Initially, I found the beginning somewhat tedious and opted for a non-linear approach, selecting topics of interest at random rather than reading from start to finish as one might with a novel. This strategy seemed fitting given the book\u0026rsquo;s comprehensive coverage of software system design, akin to an engineering bible. Today, I\u0026rsquo;ve chosen to delve into the concept of Event Sourcing. Let\u0026rsquo;s explore this topic together.\u003c/p\u003e","title":"Event Sourcing - Moving out of traditions | Simplified version"},{"content":"Objective The goal is to create a class capable of handling browser history operations efficiently. This includes:\nInitializing the browser with a specified homepage. Navigating to new URLs (visiting pages). Enabling backward and forward navigation through the history. Key Components Constructor: Initializes the browser with a homepage. Visit(URL): Navigates to a new URL and updates the current position in the history. Back(steps): Moves back a specified number of steps in history and returns the current URL. Forward(steps): Moves forward a specified number of steps in history and returns the current URL. Implementation We will use a doubly linked list to manage the history of visited URLs, allowing for efficient navigation both backward and forward.\n`class BrowserHistory { class Node { String url; Node prev, next; public Node(String url) { this.url = url; } } private Node current; public BrowserHistory(String homepage) { current = new Node(homepage); } public void visit(String url) { Node newNode = new Node(url); current.next = newNode; newNode.prev = current; current = newNode; // Move forward to the new page } public String back(int steps) { while (current.prev != null \u0026amp;\u0026amp; steps-- \u0026gt; 0) { current = current.prev; } return current.url; } public String forward(int steps) { while (current.next != null \u0026amp;\u0026amp; steps-- \u0026gt; 0) { current = current.next; } return current.url; } }` Example Usage `BrowserHistory browserHistory = new BrowserHistory(\u0026#34;takeuforward.org\u0026#34;); browserHistory.visit(\u0026#34;google.com\u0026#34;); // User visits \u0026#39;google.com\u0026#39;. browserHistory.visit(\u0026#34;instagram.com\u0026#34;); // User then visits \u0026#39;instagram.com\u0026#39;. browserHistory.back(1); // User goes back one step to \u0026#39;google.com\u0026#39;. browserHistory.back(1); // User goes back another step to \u0026#39;takeuforward.org\u0026#39;. browserHistory.forward(1); // User moves forward to \u0026#39;google.com\u0026#39; again. browserHistory.visit(\u0026#34;takeuforward.org\u0026#34;); // User visits \u0026#39;takeuforward.org\u0026#39;, overwriting forward history. browserHistory.forward(2); // No forward history, remains on \u0026#39;takeuforward.org\u0026#39;. browserHistory.back(2); // User goes back to \u0026#39;google.com\u0026#39;. browserHistory.back(7); // Attempts to go back 7 steps but only goes back to the homepage.` Complexity Analysis Constructor: O(1) - Only involves initializing a single node. Visit(URL): O(1) - Adding a new node to a doubly linked list is a constant time operation. Back(steps) and Forward(steps): O(steps) - Proportional to the number of steps taken, due to traversal through the linked list. This design mirrors the functionality of real web browsers, offering an intuitive navigation experience while ensuring operations are performed efficiently.\n","permalink":"http://localhost:1313/2024/02/designing-browser-history-feature.html","summary":"\u003ch3 id=\"objective\"\u003eObjective\u003c/h3\u003e\n\u003cp\u003eThe goal is to create a class capable of handling browser history operations efficiently. This includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitializing the browser with a specified homepage.\u003c/li\u003e\n\u003cli\u003eNavigating to new URLs (visiting pages).\u003c/li\u003e\n\u003cli\u003eEnabling backward and forward navigation through the history.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"key-components\"\u003eKey Components\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConstructor\u003c/strong\u003e: Initializes the browser with a homepage.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisit(URL)\u003c/strong\u003e: Navigates to a new URL and updates the current position in the history.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBack(steps)\u003c/strong\u003e: Moves back a specified number of steps in history and returns the current URL.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eForward(steps)\u003c/strong\u003e: Moves forward a specified number of steps in history and returns the current URL.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"implementation\"\u003eImplementation\u003c/h3\u003e\n\u003cp\u003eWe will use a doubly linked list to manage the history of visited URLs, allowing for efficient navigation both backward and forward.\u003c/p\u003e","title":"Designing a Browser History Feature"},{"content":"On the early morning of February 1st, I decided to tackle a problem from the Striver\u0026rsquo;s SDE sheet.\nI came across an intriguing challenge titled \u0026ldquo;Set Matrix Zeroes\u0026rdquo;.\nHere\u0026rsquo;s how I approached it:\nInitially, I attempted a brute force method. The usual strategy of iterating through a matrix and setting rows and columns to zero has a drawback: it can lead to the loss of the matrix\u0026rsquo;s original state, potentially zeroing out the entire matrix, which isn\u0026rsquo;t the desired outcome. To avoid this, my approach involved marking the rows and columns that intersect at a zero element with a temporary value of -1. This way, the matrix\u0026rsquo;s original elements remain unchanged until the final step, where a second pass converts all -1 values back to zeros.\nThe main drawback of this method is its inefficiency, with a time complexity that could be roughly estimated as (O((m times n)^2)), due to the need for multiple iterations over the matrix.\nSeeking improvement, I explored an optimal solution that builds upon the brute force idea but introduces two marker arrays, one for rows and another for columns. As we scan the original matrix, we use these arrays to mark any row or column containing a zero. A subsequent iteration through the matrix then uses these markers to set the corresponding elements to zero. This refined approach reduces the number of iterations, achieving a time complexity of (O(2 times m times n)).\nDespite the improvement, I wondered if there was an even better solution. Recalling that I had previously solved this problem on LeetCode under the title \u0026ldquo;Set Matrix Zeroes\u0026rdquo; (you can find the problem [here](https://leetcode.com/problems/set-matrix-zeroes/)), I revisited my submission. To my surprise, I had employed a different, more efficient strategy in my past solution, which can be viewed [here](https://leetcode.com/problems/set-matrix-zeroes/submissions/1004912402/).\nThe best solution I found leverages the first row and first column of the matrix as marker arrays, eliminating the need for additional space. A special variable is used to determine whether the first row or column should be entirely zeroed. This approach requires careful iteration from the second row and column onwards, with a final pass to address the first row and column as needed.\nFor those interested in the solutions, they are available on my GitHub repository at [this link](https://github.com/mathewjustin/2024-DsAndAlgo/tree/main/src/main/java/com/codepower/striver/arrays).\nThank you for taking the time to read this if you\u0026rsquo;ve made it this far.\nJustin\n","permalink":"http://localhost:1313/2024/01/an-early-morning-adventure.html","summary":"\u003cp\u003eOn the early morning of February 1st, I decided to tackle a problem from the Striver\u0026rsquo;s SDE sheet.\u003c/p\u003e\n\u003cp\u003eI came across an intriguing challenge titled \u0026ldquo;Set Matrix Zeroes\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s how I approached it:\u003c/p\u003e\n\u003cp\u003eInitially, I attempted a brute force method. The usual strategy of iterating through a matrix and setting rows and columns to zero has a drawback: it can lead to the loss of the matrix\u0026rsquo;s original state, potentially zeroing out the entire matrix, which isn\u0026rsquo;t the desired outcome. To avoid this, my approach involved marking the rows and columns that intersect at a zero element with a temporary value of -1. This way, the matrix\u0026rsquo;s original elements remain unchanged until the final step, where a second pass converts all -1 values back to zeros.\u003c/p\u003e","title":"An early morning adventure"},{"content":"KEEP ONE HAND DISTANCE!\n","permalink":"http://localhost:1313/2022/04/an-engineers-view-to-religion.html","summary":"\u003cp\u003eKEEP ONE HAND DISTANCE!\u003c/p\u003e","title":"An engineers view to religion"},{"content":"Cycle travelled all the way from Bangalore to Kannan devan hills, Idukki. ;) A wonderful journey of last 3 years has come to an end. Moved out of MBRDI / DTICI ( Daimler Trucks Innovation Center India)\n","permalink":"http://localhost:1313/2021/12/moving-from-wonderful-organiztion.html","summary":"\u003cp\u003eCycle travelled all the way from Bangalore to Kannan devan hills, Idukki. ;) \u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhf3De-WXVocLoeSz6TAH3rhUUqwui_q2DBZMiUYpKx6f__tWEfmn1ctsLAO_q6KSmGrE_u8QNvDwAlDTdTVn8ayBInEF-V01eVkMnLmtlVcc1xonfZJ4JncASVF9TzY4ESGAgeSZF1b6JT1RrcuuLM3XcM04IsebMAas5NMjDdFnnWpfa-1krWSiWKPg=s1024\"\u003e\u003cimg loading=\"lazy\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhf3De-WXVocLoeSz6TAH3rhUUqwui_q2DBZMiUYpKx6f__tWEfmn1ctsLAO_q6KSmGrE_u8QNvDwAlDTdTVn8ayBInEF-V01eVkMnLmtlVcc1xonfZJ4JncASVF9TzY4ESGAgeSZF1b6JT1RrcuuLM3XcM04IsebMAas5NMjDdFnnWpfa-1krWSiWKPg=w379-h379\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA wonderful journey of last 3 years has come to an end. Moved out of MBRDI / DTICI ( Daimler Trucks Innovation Center India)\u003c/p\u003e","title":"Moving from a wonderful organiztion !"},{"content":" I like to fork interesting github repositories and experiment with it. Those who do this actively might have faced the problem of upstream sync, where we need to sync our repository with the upstream repository. Imagine you forked a repository contains some articles which are being updated everyday, you will like them to have it synced everyday. I do ;) Github introduced a feature called fetch upstream this year to solve this issue. But in this case you need to click on the fetch upstream button by yourself, this article is for the lazy ones who want it to be done automatically. In this blog we are about to learn how to automate upstream fetch of any repository you cloned. We are using Github actions to do this. The below is the yaml configuration for my github action\nHere you can see the cron is set to run at midnight, also as you might have already noticed i have created a simple bat file to be excecuted, you can see it below. Next step is to configure a github action with the yml file we created. Go to Actions tab\nClick on New Workflow-\u0026gt; Skip this and set up workflow yourself.\nHere you can select the yml file you created under your repository. Once you set it it up your fork will automatically sync every night. You can see this example live on my github repository : every-programmer-should-know\n","permalink":"http://localhost:1313/2021/08/automate-upstream-update-using-github.html","summary":"\u003cp\u003e      I like to fork interesting github repositories and experiment with it. Those who do this actively might have faced the problem of upstream sync, where we need to sync our repository with the upstream repository. Imagine you forked a repository contains some articles which are being updated everyday, you will like them to have it synced everyday. I do ;) \u003c/p\u003e\n\u003cp\u003e  Github introduced a feature called \u003ca href=\"https://docs.github.com/en/github/collaborating-with-pull-requests/working-with-forks/syncing-a-fork\"\u003efetch upstream\u003c/a\u003e this year to solve this issue. But in this case you need to click on the fetch upstream button by yourself, this article is for the lazy ones who want it to be done automatically. \u003c/p\u003e","title":"Automate fetch upstream using github actions"},{"content":"\nYou can make a great software but you have to choose 2 out of this above 3. There is no way to get around!\n","permalink":"http://localhost:1313/2021/05/cheap-fast-good.html","summary":"\u003cp\u003e\u003ca href=\"https://live.staticflickr.com/3055/2647397230_16b0828cc1.jpg\"\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/3055/2647397230_16b0828cc1.jpg\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eYou can make a great software but you have to choose 2 out of this above 3. There is no way to get around!\u003c/strong\u003e\u003c/p\u003e","title":"Cheap Fast Good"},{"content":" class Node: def \\_\\_init\\_\\_(self, data): self.data=data self.nextNode=None class LinkedLIst: def \\_\\_init\\_\\_(self): self.head = None self.numberOFNodes=0 \\# Here we get o(1) constant running time complexity for insertion. def insert\\_start(self,data): self.numberOFNodes=self.numberOFNodes+1 new\\_node = Node(data) if not self.head: self.head=new\\_node else: new\\_node.nextNode=self.head self.head = new\\_node #Linear running time o(n) def insert\\_end(self,data): self.numberOFNodes=self.numberOFNodes+1 new\\_node=Node(data) actual\\_node=self.head while actual\\_node.nextNode is not None: actual\\_node=actual\\_node.nextNode actual\\_node.nextNode=new\\_node def size\\_of\\_list(self): actual\\_node = self.head while actual\\_node is not None: print(actual\\_node) actual\\_node=actual\\_node.nextNode def traverse(self): actual\\_node=self.head while actual\\_node is not None: print(actual\\_node.data) actual\\_node=actual\\_node.nextNode def deleteAtHead(self): self.head=self.head.nextNode self.numberOFNodes=self.numberOFNodes-1 def deleteAtTail(self): actual\\_node = self.head while actual\\_node.nextNode.nextNode is not None: actual\\_node=actual\\_node.nextNode actual\\_node.nextNode=None self.numberOFNodes=self.numberOFNodes-1 linked\\_list=LinkedLIst() linked\\_list.insert\\_start(4) linked\\_list.insert\\_start(3) linked\\_list.insert\\_start(\u0026#39;A String type\u0026#39;) linked\\_list.insert\\_start(1.232) linked\\_list.insert\\_end(12.232) linked\\_list.traverse() print(\u0026#39;Deleting at the front\u0026#39;) linked\\_list.deleteAtHead() linked\\_list.traverse() print(\u0026#39;Delete at the last position\u0026#39;) linked\\_list.deleteAtTail() linked\\_list.traverse() ","permalink":"http://localhost:1313/2021/01/python-linked-list-implementation.html","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e  \nclass Node:  \n     def \\_\\_init\\_\\_(self, data):  \n         self.data=data  \n         self.nextNode=None  \n  \n  \n  \nclass LinkedLIst:  \n  \n    def \\_\\_init\\_\\_(self):  \n        self.head = None  \n  self.numberOFNodes=0  \n  \\# Here we get o(1) constant running time complexity for insertion.  \n  def insert\\_start(self,data):  \n  \n        self.numberOFNodes=self.numberOFNodes+1  \n  new\\_node = Node(data)  \n  \n        if not self.head:  \n            self.head=new\\_node  \n        else:  \n            new\\_node.nextNode=self.head  \n            self.head = new\\_node  \n #Linear running time o(n)  \n  def insert\\_end(self,data):  \n        self.numberOFNodes=self.numberOFNodes+1  \n  new\\_node=Node(data)  \n  \n        actual\\_node=self.head  \n  \n        while actual\\_node.nextNode is not None:  \n            actual\\_node=actual\\_node.nextNode  \n  \n        actual\\_node.nextNode=new\\_node  \n  \n    def size\\_of\\_list(self):  \n  \n        actual\\_node = self.head  \n  \n        while actual\\_node is not None:  \n            print(actual\\_node)  \n            actual\\_node=actual\\_node.nextNode  \n  \n    def traverse(self):  \n        actual\\_node=self.head  \n  \n        while actual\\_node is not None:  \n            print(actual\\_node.data)  \n            actual\\_node=actual\\_node.nextNode  \n  \n    def deleteAtHead(self):  \n  \n        self.head=self.head.nextNode  \n        self.numberOFNodes=self.numberOFNodes-1  \n  \n  def deleteAtTail(self):  \n  \n        actual\\_node = self.head  \n  \n        while actual\\_node.nextNode.nextNode is not None:  \n            actual\\_node=actual\\_node.nextNode  \n  \n        actual\\_node.nextNode=None  \n  self.numberOFNodes=self.numberOFNodes-1  \n  \n  \n  \nlinked\\_list=LinkedLIst()  \nlinked\\_list.insert\\_start(4)  \nlinked\\_list.insert\\_start(3)  \nlinked\\_list.insert\\_start(\u0026#39;A String type\u0026#39;)  \nlinked\\_list.insert\\_start(1.232)  \nlinked\\_list.insert\\_end(12.232)  \nlinked\\_list.traverse()  \n  \nprint(\u0026#39;Deleting at the front\u0026#39;)  \nlinked\\_list.deleteAtHead()  \nlinked\\_list.traverse()  \n  \nprint(\u0026#39;Delete at the last position\u0026#39;)  \nlinked\\_list.deleteAtTail()  \nlinked\\_list.traverse()\n\u003c/code\u003e\u003c/pre\u003e","title":"Python linked List implementation"},{"content":" I had worked on spring framework extensively throughout my career. The one thing which I am always try to implement in any Spring project I work on is Custom annotation. Its a fancy stuff but useful in many ways. Say you have numerous micro services (Duplicate code bases which will f*** you up) running on your cluster, the best and first thing you do is - Build a common library to push all your model classes, so called util package of your organization :p and the fancy stuffs . I am fortunate that I get to do all these experiments early on my career. So lets start with the problem.\nWe have many micro services which are built on top of spring framework. We want to use a common service bean in all our boot apps. This bean should be enabled with @EnableMyBean annotation. Once the spring boot app loads it should intercept this annotation and inject our MyBean service to our Spring boot application. So the first step to do is create the annotation itself\nThe @Import is from org.springframework.context.annotation package You can find the documentation over here\nNext is the implementation of MyBeanSelector class. We can define it as follows.\nLets look at each components. First the importSelector interface. As the definition on the Spring document \u0026ldquo;Interface to be implemented by types that determine which @Configuration__ class(es) should be imported based on a given selection criteria, usually one or more annotation attributes.\u0026rdquo; This class is accountable for which Bean should be injected to the spring context. It has many other cool features as well. Say you want to read some property from the environment in which the common library is used. Ie, from the Yaml file of the spring boot app which is using your common library. Then you can implement EnvironmentAware Interface. Once we do the above, next we can define the actual service class inside \u0026ldquo;com.commons.service\u0026rdquo;. So now You can use @EnableMyBean on your main method of the spring boot application. And you can Autowire MyService class anywhere inside your application. At the time of start up spring would automatically detect the @EnableMyBean annotation and push the Service Bean to the app\u0026rsquo;s context.\n","permalink":"http://localhost:1313/2020/10/custom-annotations-in-spring-boot.html","summary":"\u003cp\u003e           I had worked on spring framework extensively throughout my career. The one thing which I am always try to implement in any Spring project I work on is Custom annotation. Its a fancy stuff but useful in many ways. \u003c/p\u003e\n\u003cp\u003e         Say you have numerous micro services (Duplicate code bases which will f*** you up) running on your cluster, the best and first thing you do is - Build a common library to push all your model classes, so called util package of your organization :p and the fancy stuffs  . I am fortunate that I get to do all these experiments early on my career. So lets start with the problem.\u003c/p\u003e","title":"Custom annotations in Spring Boot"},{"content":" When comes to any programming language what i check first is how to write a unit test on it. \u0026ldquo;Coding is not difficult- Bill Gates\u0026rdquo; So what is difficult? I would say testing is difficult. Here is a basic example on how to test a small piece of python code.\nThe above snippet is a basic recursive program which will return the factorial of a number. Lets see how we can write a basic test case for this. I have my directory structure as follows.\nLets see the contents of n_factorial_tests.py.\nThe first line is on importing the program to the test class. Next we are importing _unittest : T_his module provides a rich set of tools for constructing and running tests. While defining a function you can see we have used an object \u0026ldquo;self\u0026rdquo;. This used to represent an instance of a class. Here it means accessing the contents of unittest.TestCase which is being inherited to this class. Now we can access the functions defined unittest.TestCase module. Because we have inherited that to our test class. self.assertEqual(n_factorial.factorial(5),120) : This is the statement where we assert for our expectation vs the actual value. If you run this on PyCharm we can see the below. Finally i wrote a python code and tested it ❤️\n","permalink":"http://localhost:1313/2020/09/what-i-found-on-python-basics-1.html","summary":"\u003cp\u003e When comes to any programming language what i check first is how to write a unit test on it. \u003c/p\u003e\n\u003cp\u003e   \u003cem\u003e\u0026ldquo;Coding is not difficult- Bill Gates\u0026rdquo;\u003c/em\u003e So what is difficult? I would say testing is difficult. Here is a basic example on how to test a small piece of python code.\u003c/p\u003e\n\u003cp\u003e                The above snippet is a basic recursive program which will return the factorial of a number. Lets see how we can write a basic test case for this. I have my directory structure as follows.\u003c/p\u003e","title":"Python Unit Test: Basics 1"}]